{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2025-05-01 00:07:06   2025-05-01 00:24:15              1.0   \n",
      "1         2  2025-05-01 00:07:44   2025-05-01 00:14:27              1.0   \n",
      "2         2  2025-05-01 00:15:56   2025-05-01 00:23:53              1.0   \n",
      "3         2  2025-05-01 00:00:09   2025-05-01 00:25:29              1.0   \n",
      "4         2  2025-05-01 00:45:07   2025-05-01 00:52:45              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           3.70         1.0                  N           140           202   \n",
      "1           1.03         1.0                  N           234           161   \n",
      "2           1.57         1.0                  N           161           234   \n",
      "3           9.48         1.0                  N           138            90   \n",
      "4           1.80         1.0                  N            90           231   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             1         18.4   4.25      0.5        4.85          0.00   \n",
      "1             1          8.6   1.00      0.5        4.30          0.00   \n",
      "2             2         10.0   1.00      0.5        0.00          0.00   \n",
      "3             1         40.8   6.00      0.5       11.70          6.94   \n",
      "4             1         10.0   1.00      0.5        1.50          0.00   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \\\n",
      "0                    1.0         29.00                   2.5         0.00   \n",
      "1                    1.0         18.65                   2.5         0.00   \n",
      "2                    1.0         15.75                   2.5         0.00   \n",
      "3                    1.0         71.94                   2.5         1.75   \n",
      "4                    1.0         17.25                   2.5         0.00   \n",
      "\n",
      "   cbd_congestion_fee  \n",
      "0                0.75  \n",
      "1                0.75  \n",
      "2                0.75  \n",
      "3                0.75  \n",
      "4                0.75  \n",
      "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
      "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
      "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
      "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
      "       'total_amount', 'congestion_surcharge', 'Airport_fee',\n",
      "       'cbd_congestion_fee'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Replace with your local file path\n",
    "# parquet_file = \"/Users/charlessanthakumar/Documents/POC/yellow_tripdata_2025-05.parquet\"\n",
    "\n",
    "# # Read Parquet file\n",
    "# df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# # View the first 5 rows\n",
    "# print(df.head())\n",
    "\n",
    "# # View columns\n",
    "# print(df.columns)\n",
    "\n",
    "# # View basic info\n",
    "# #print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = \"taxi_zone_lookup.csv\"\n",
    "\n",
    "# df_csv = pd.read_csv(csv_file)\n",
    "# print(\"\\nFirst 5 rows of the CSV file:\")\n",
    "# print(df_csv.head())\n",
    "# print(\"\\nCSV Columns:\")\n",
    "# print(df_csv.columns)\n",
    "# print(\"\\nCSV Info:\")\n",
    "# # print(df_csv.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37486c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   date datatype            station attributes  value\n",
      "0   2025-05-01T00:00:00     AWND  GHCND:USW00094728       ,,W,    2.4\n",
      "1   2025-05-01T00:00:00     PRCP  GHCND:USW00094728   ,,W,2400    0.0\n",
      "2   2025-05-01T00:00:00     SNOW  GHCND:USW00094728       ,,W,    0.0\n",
      "3   2025-05-01T00:00:00     SNWD  GHCND:USW00094728   ,,W,2400    0.0\n",
      "4   2025-05-01T00:00:00     TMAX  GHCND:USW00094728   ,,W,2400   21.7\n",
      "..                  ...      ...                ...        ...    ...\n",
      "95  2025-05-09T00:00:00     WDF5  GHCND:USW00094728       ,,W,   60.0\n",
      "96  2025-05-09T00:00:00     WSF2  GHCND:USW00094728       ,,W,    7.2\n",
      "97  2025-05-09T00:00:00     WSF5  GHCND:USW00094728       ,,W,   10.7\n",
      "98  2025-05-09T00:00:00     WT01  GHCND:USW00094728       ,,W,    1.0\n",
      "99  2025-05-10T00:00:00     AWND  GHCND:USW00094728       ,,W,    2.5\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "Index(['date', 'datatype', 'station', 'attributes', 'value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "#import pandas as pd\n",
    "\n",
    "# Replace with your NOAA API token\n",
    "TOKEN = \"uuhabihGBDGcWfSkBUcWIraYGqSscifq\"\n",
    "\n",
    "# Define endpoint and parameters\n",
    "endpoint = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "\n",
    "headers = {\"token\": TOKEN}\n",
    "\n",
    "# Example: Fetch daily summaries for Central Park Station (USW00094728)\n",
    "params = {\n",
    "    \"datasetid\": \"GHCND\",\n",
    "    \"stationid\": \"GHCND:USW00094728\",\n",
    "    \"startdate\": \"2025-05-01\",\n",
    "    \"enddate\": \"2025-05-31\",\n",
    "    \"units\": \"metric\",\n",
    "    \"limit\": 1000\n",
    "}\n",
    "\n",
    "response = requests.get(endpoint, headers=headers, params=params)\n",
    "output_path = \"weather_data_pivoted.csv\"  # or \"nyc_taxi_weather_enriched.csv\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    df = pd.json_normalize(data['results'])\n",
    "    print(df.head(100))\n",
    "    print(df.columns)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "                    date datatype            station attributes  value\n",
      "0    2025-05-01T00:00:00     AWND  GHCND:USW00094728       ,,W,    2.4\n",
      "1    2025-05-01T00:00:00     PRCP  GHCND:USW00094728   ,,W,2400    0.0\n",
      "2    2025-05-01T00:00:00     SNOW  GHCND:USW00094728       ,,W,    0.0\n",
      "3    2025-05-01T00:00:00     SNWD  GHCND:USW00094728   ,,W,2400    0.0\n",
      "4    2025-05-01T00:00:00     TMAX  GHCND:USW00094728   ,,W,2400   21.7\n",
      "..                   ...      ...                ...        ...    ...\n",
      "331  2025-05-31T00:00:00     WDF2  GHCND:USW00094728       ,,W,  290.0\n",
      "332  2025-05-31T00:00:00     WDF5  GHCND:USW00094728       ,,W,  300.0\n",
      "333  2025-05-31T00:00:00     WSF2  GHCND:USW00094728       ,,W,    8.1\n",
      "334  2025-05-31T00:00:00     WSF5  GHCND:USW00094728       ,,W,   16.1\n",
      "335  2025-05-31T00:00:00     WT01  GHCND:USW00094728       ,,W,    1.0\n",
      "\n",
      "[336 rows x 5 columns]\n",
      "\n",
      "Pivoted Data:\n",
      "                   date  AWND  PRCP  SNOW  SNWD  TMAX  TMIN   WDF2   WDF5  \\\n",
      "0   2025-05-01T00:00:00   2.4   0.0   0.0   0.0  21.7  11.1  130.0  130.0   \n",
      "1   2025-05-02T00:00:00   1.0   0.0   0.0   0.0  28.9  13.9  230.0  240.0   \n",
      "2   2025-05-03T00:00:00   1.3  24.1   0.0   0.0  29.4  17.2   60.0  220.0   \n",
      "3   2025-05-04T00:00:00   1.3   3.8   0.0   0.0  20.6  15.6  180.0  180.0   \n",
      "4   2025-05-05T00:00:00   2.9  18.8   0.0   0.0  16.1  13.9   70.0   70.0   \n",
      "5   2025-05-06T00:00:00   1.6   6.1   0.0   0.0  17.8  13.9  130.0  130.0   \n",
      "6   2025-05-07T00:00:00   0.7   0.5   0.0   0.0  23.3  12.8  220.0  230.0   \n",
      "7   2025-05-08T00:00:00   2.1   0.0   0.0   0.0  25.6  15.6   50.0  360.0   \n",
      "8   2025-05-09T00:00:00   3.2  17.5   0.0   0.0  15.6  10.6   70.0   60.0   \n",
      "9   2025-05-10T00:00:00   2.5   0.0   0.0   0.0  22.2  10.0  300.0  300.0   \n",
      "10  2025-05-11T00:00:00   1.3   0.0   0.0   0.0  25.0  14.4  300.0  290.0   \n",
      "11  2025-05-12T00:00:00   1.2   0.0   0.0   0.0  22.2  13.9  110.0  100.0   \n",
      "12  2025-05-13T00:00:00   1.4   4.1   0.0   0.0  20.6  14.4  130.0  130.0   \n",
      "13  2025-05-14T00:00:00   2.8  31.0   0.0   0.0  16.7  13.3  100.0   70.0   \n",
      "14  2025-05-15T00:00:00   1.8   0.0   0.0   0.0  20.6  15.0   60.0   50.0   \n",
      "15  2025-05-16T00:00:00   0.6   2.8   0.0   0.0  22.2  16.1  140.0  150.0   \n",
      "16  2025-05-17T00:00:00   1.8   0.0   0.0   0.0  27.8  17.2  300.0  290.0   \n",
      "17  2025-05-18T00:00:00   4.1   0.0   0.0   0.0  22.8  16.1  280.0  330.0   \n",
      "18  2025-05-19T00:00:00   3.8   0.0   0.0   0.0  20.6  10.6  300.0  270.0   \n",
      "19  2025-05-20T00:00:00   2.6   0.0   0.0   0.0  19.4   9.4   40.0   10.0   \n",
      "20  2025-05-21T00:00:00   2.5   2.3   0.0   0.0  15.0   9.4   70.0  100.0   \n",
      "21  2025-05-22T00:00:00   3.5  22.9   0.0   0.0  10.6   8.9   50.0   30.0   \n",
      "22  2025-05-23T00:00:00   0.6   6.6   0.0   0.0  17.2   8.3  280.0  280.0   \n",
      "23  2025-05-24T00:00:00   2.6   0.0   0.0   0.0  17.8   8.9  300.0  310.0   \n",
      "24  2025-05-25T00:00:00   2.2   0.0   0.0   0.0  18.9  11.1  270.0  290.0   \n",
      "25  2025-05-26T00:00:00   1.7   0.0   0.0   0.0  22.8  11.1   40.0   50.0   \n",
      "26  2025-05-27T00:00:00   0.9   0.0   0.0   0.0  23.3  13.9  220.0  170.0   \n",
      "27  2025-05-28T00:00:00   1.4  12.4   0.0   0.0  18.3  11.1   70.0    NaN   \n",
      "28  2025-05-29T00:00:00   1.6   2.5   0.0   0.0  19.4  12.8   60.0  200.0   \n",
      "29  2025-05-30T00:00:00   1.6   0.0   0.0   0.0  23.9  16.7  230.0  250.0   \n",
      "30  2025-05-31T00:00:00   4.0  11.7   0.0   0.0  19.4  11.7  290.0  300.0   \n",
      "\n",
      "    WSF2  WSF5  WT01  WT02  WT03  WT08  \n",
      "0    5.8   9.4   NaN   NaN   NaN   NaN  \n",
      "1    6.3  10.3   1.0   NaN   NaN   1.0  \n",
      "2    7.2  10.7   1.0   1.0   1.0   NaN  \n",
      "3    4.0   8.9   1.0   NaN   NaN   NaN  \n",
      "4    6.7   9.8   1.0   NaN   NaN   NaN  \n",
      "5    5.8  10.7   1.0   NaN   NaN   NaN  \n",
      "6    4.0   7.6   NaN   NaN   NaN   NaN  \n",
      "7    5.4   8.9   NaN   NaN   NaN   NaN  \n",
      "8    7.2  10.7   1.0   NaN   NaN   NaN  \n",
      "9    7.6  14.8   NaN   NaN   NaN   NaN  \n",
      "10   5.8   9.8   NaN   NaN   NaN   NaN  \n",
      "11   4.0   7.6   NaN   NaN   NaN   NaN  \n",
      "12   5.4   9.4   1.0   NaN   NaN   NaN  \n",
      "13   5.8   9.4   1.0   NaN   NaN   NaN  \n",
      "14   4.5   5.8   1.0   NaN   NaN   1.0  \n",
      "15   5.8   9.8   1.0   NaN   NaN   1.0  \n",
      "16   7.2  13.0   1.0   NaN   NaN   1.0  \n",
      "17   7.6  14.3   NaN   NaN   NaN   NaN  \n",
      "18   7.6  14.8   NaN   NaN   NaN   NaN  \n",
      "19   6.3  10.3   NaN   NaN   NaN   NaN  \n",
      "20   6.7  12.1   1.0   NaN   NaN   1.0  \n",
      "21   7.6  11.2   1.0   NaN   NaN   NaN  \n",
      "22   5.8   9.4   1.0   NaN   NaN   1.0  \n",
      "23   7.2  13.0   NaN   NaN   NaN   NaN  \n",
      "24   5.8  10.3   NaN   NaN   NaN   NaN  \n",
      "25   5.4   8.1   NaN   NaN   NaN   NaN  \n",
      "26   3.1   6.3   NaN   NaN   NaN   NaN  \n",
      "27   4.0   NaN   1.0   NaN   NaN   NaN  \n",
      "28   4.5   8.9   1.0   NaN   NaN   1.0  \n",
      "29   4.5   8.1   1.0   NaN   NaN   1.0  \n",
      "30   8.1  16.1   1.0   NaN   NaN   NaN  \n",
      "\n",
      "Pivoted data saved to weather_pivoted.csv\n"
     ]
    }
   ],
   "source": [
    "# #import pandas as pd\n",
    "\n",
    "# # Read your CSV\n",
    "# df = pd.read_csv(\"weather_data_pivoted.csv\")\n",
    "\n",
    "# # Display the original DataFrame\n",
    "# print(\"Original Data:\")\n",
    "# print(df)\n",
    "\n",
    "# # Keep only relevant columns\n",
    "# df_clean = df[['date', 'datatype', 'value']]\n",
    "\n",
    "# # Pivot the datatype into columns, values filled from 'value'\n",
    "# df_pivot = df_clean.pivot_table(\n",
    "#     index='date',\n",
    "#     columns='datatype',\n",
    "#     values='value'\n",
    "# ).reset_index()\n",
    "\n",
    "# # Flatten column MultiIndex if needed\n",
    "# df_pivot.columns.name = None\n",
    "\n",
    "# # Display the pivoted DataFrame\n",
    "# print(\"\\nPivoted Data:\")\n",
    "# print(df_pivot)\n",
    "\n",
    "# # Save to CSV if desired\n",
    "# df_pivot.to_csv(\"weather_pivoted.csv\", index=False)\n",
    "# print(\"\\nPivoted data saved to weather_pivoted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83494659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import pandas as pd\n",
    "# class NYCTripDataProcessor:\n",
    "#     def __init__(self, file_path):\n",
    "#         self.file_path = file_path\n",
    "#         self.df = pd.read_parquet(file_path)\n",
    "\n",
    "#     def get_trip_data(self):\n",
    "#         return self.df\n",
    "\n",
    "#     def get_columns(self):\n",
    "#         return self.df.columns.tolist()\n",
    "\n",
    "#     def get_info(self):\n",
    "#         return self.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d786d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataProcessor:\n",
    "    def __init__(self, token, endpoint):\n",
    "        self.token = token\n",
    "        self.endpoint = endpoint\n",
    "        self.headers = {\"token\": self.token}\n",
    "        \n",
    "    def fetch_weather_data(self, dataset_id, station_id, start_date, end_date, units):\n",
    "        params = {\n",
    "            \"datasetid\": dataset_id,\n",
    "            \"stationid\": station_id,\n",
    "            \"startdate\": start_date,\n",
    "            \"enddate\": end_date,\n",
    "            \"units\": units,\n",
    "            #\"limit\": 1000\n",
    "        }\n",
    "        response = requests.get(self.endpoint, headers=self.headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return pd.json_normalize(data['results'])\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "        \n",
    "    def pivot_weather_data(self, df):\n",
    "        df_clean = df[['date', 'datatype', 'value']]\n",
    "        df_pivot = df_clean.pivot_table(\n",
    "            index='date',\n",
    "            columns='datatype',\n",
    "            values='value'\n",
    "        ).reset_index()\n",
    "        df_pivot.columns.name = None\n",
    "        return df_pivot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9874e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #not to be considered\n",
    "# class DQChecker:\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    "\n",
    "#     def check_nulls(self):\n",
    "#         return self.df.isnull().sum()\n",
    "\n",
    "#     def check_duplicates(self):\n",
    "#         return self.df.duplicated().sum()\n",
    "\n",
    "#     def check_data_types(self):\n",
    "#         return self.df.dtypes\n",
    "\n",
    "#     def run_all_checks(self):\n",
    "#         return {\n",
    "#             \"nulls\": self.check_nulls(),\n",
    "#             \"duplicates\": self.check_duplicates(),\n",
    "#             \"data_types\": self.check_data_types()\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e134fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils:\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_to_csv(df, file_name):\n",
    "        df.to_csv(file_name, index=False)\n",
    "        print(f\"Data saved to {file_name}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_csv(file_name):\n",
    "        return pd.read_csv(file_name)\n",
    "    @staticmethod\n",
    "    def derive_date_columns(df, date_column, new_column_name):\n",
    "        # derive date from timestamp and append to the DataFrame\n",
    "        df[new_column_name] = pd.to_datetime(df[date_column]).dt.date\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching trip data...\n",
      "Trip Data Columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee']\n",
      "Trip Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4591845 entries, 0 to 4591844\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int32         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int32         \n",
      " 8   DOLocationID           int32         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  Airport_fee            float64       \n",
      " 19  cbd_congestion_fee     float64       \n",
      "dtypes: datetime64[us](2), float64(13), int32(3), int64(1), object(1)\n",
      "memory usage: 648.1+ MB\n",
      "\n",
      "Fetching weather data from API...\n",
      "Sample data from Weather Dataset\n",
      "                   date datatype            station attributes  value\n",
      "0  2025-05-01T00:00:00     AWND  GHCND:USW00094728       ,,W,    2.4\n",
      "1  2025-05-01T00:00:00     PRCP  GHCND:USW00094728   ,,W,2400    0.0\n",
      "2  2025-05-01T00:00:00     SNOW  GHCND:USW00094728       ,,W,    0.0\n",
      "3  2025-05-01T00:00:00     SNWD  GHCND:USW00094728   ,,W,2400    0.0\n",
      "4  2025-05-01T00:00:00     TMAX  GHCND:USW00094728   ,,W,2400   21.7\n",
      "Weather Data Columns: Index(['date', 'datatype', 'station', 'attributes', 'value'], dtype='object')\n",
      "Pivoting weather data...\n",
      "Pivoted Weather Data:\n",
      "                  date  AWND  PRCP  SNOW  SNWD  TMAX  TMIN   WDF2   WDF5  \\\n",
      "0  2025-05-01T00:00:00   2.4   0.0   0.0   0.0  21.7  11.1  130.0  130.0   \n",
      "1  2025-05-02T00:00:00   1.0   0.0   0.0   0.0  28.9  13.9  230.0  240.0   \n",
      "2  2025-05-03T00:00:00   1.3  24.1   0.0   NaN   NaN   NaN    NaN    NaN   \n",
      "\n",
      "   WSF2  WSF5  WT01  WT08  \n",
      "0   5.8   9.4   NaN   NaN  \n",
      "1   6.3  10.3   1.0   1.0  \n",
      "2   NaN   NaN   NaN   NaN  \n",
      "Deriving date column for weather data...\n",
      "Deriving date column from trip data...\n",
      "Sample data after deriving date column:\n",
      "                  date  AWND  PRCP  SNOW  SNWD  TMAX  TMIN   WDF2   WDF5  \\\n",
      "0  2025-05-01T00:00:00   2.4   0.0   0.0   0.0  21.7  11.1  130.0  130.0   \n",
      "1  2025-05-02T00:00:00   1.0   0.0   0.0   0.0  28.9  13.9  230.0  240.0   \n",
      "2  2025-05-03T00:00:00   1.3  24.1   0.0   NaN   NaN   NaN    NaN    NaN   \n",
      "\n",
      "   WSF2  WSF5  WT01  WT08 weather_date  \n",
      "0   5.8   9.4   NaN   NaN   2025-05-01  \n",
      "1   6.3  10.3   1.0   1.0   2025-05-02  \n",
      "2   NaN   NaN   NaN   NaN   2025-05-03  \n",
      "Sample data after deriving date column from trip data:\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2025-05-01 00:07:06   2025-05-01 00:24:15              1.0   \n",
      "1         2  2025-05-01 00:07:44   2025-05-01 00:14:27              1.0   \n",
      "2         2  2025-05-01 00:15:56   2025-05-01 00:23:53              1.0   \n",
      "3         2  2025-05-01 00:00:09   2025-05-01 00:25:29              1.0   \n",
      "4         2  2025-05-01 00:45:07   2025-05-01 00:52:45              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           3.70         1.0                  N           140           202   \n",
      "1           1.03         1.0                  N           234           161   \n",
      "2           1.57         1.0                  N           161           234   \n",
      "3           9.48         1.0                  N           138            90   \n",
      "4           1.80         1.0                  N            90           231   \n",
      "\n",
      "   payment_type  ...  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             1  ...      0.5        4.85          0.00   \n",
      "1             1  ...      0.5        4.30          0.00   \n",
      "2             2  ...      0.5        0.00          0.00   \n",
      "3             1  ...      0.5       11.70          6.94   \n",
      "4             1  ...      0.5        1.50          0.00   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \\\n",
      "0                    1.0         29.00                   2.5         0.00   \n",
      "1                    1.0         18.65                   2.5         0.00   \n",
      "2                    1.0         15.75                   2.5         0.00   \n",
      "3                    1.0         71.94                   2.5         1.75   \n",
      "4                    1.0         17.25                   2.5         0.00   \n",
      "\n",
      "   cbd_congestion_fee  pickup_date  dropoff_date  \n",
      "0                0.75   2025-05-01    2025-05-01  \n",
      "1                0.75   2025-05-01    2025-05-01  \n",
      "2                0.75   2025-05-01    2025-05-01  \n",
      "3                0.75   2025-05-01    2025-05-01  \n",
      "4                0.75   2025-05-01    2025-05-01  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trip_data_file_path = \"yellow_tripdata_2025-05.parquet\"\n",
    "    zone_lookup_file_path = \"taxi_zone_lookup.csv\"\n",
    "\n",
    "    weather_token = \"uuhabihGBDGcWfSkBUcWIraYGqSscifq\"\n",
    "    weather_endpoint = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "    dataset_id = \"GHCND\"\n",
    "    station_id = \"GHCND:USW00094728\"\n",
    "    start_date = \"2025-05-01\"\n",
    "    end_date = \"2025-05-31\"\n",
    "    units = \"metric\"\n",
    "\n",
    "    \n",
    "\n",
    "    # Initialize Nyc trip data processor\n",
    "    trip_data_processor = NYCTripDataProcessor(trip_data_file_path)\n",
    "    #zone_mapping = NYCTripDataProcessor(zone_lookup_file_path)\n",
    "    weather_data_processor = WeatherDataProcessor(weather_token, weather_endpoint)\n",
    "    #utils_instance = utils()\n",
    "\n",
    "    print(\"Fetching trip data...\")\n",
    "    trip_df = trip_data_processor.get_trip_data()\n",
    "    #zone_mapping_df = zone_mapping.get_trip_data()\n",
    "\n",
    "    print(\"Trip Data Columns:\", trip_data_processor.get_columns())\n",
    "    #print(\"Zone Mapping Columns:\", zone_mapping_df.get_columns())\n",
    "\n",
    "    print(\"Trip Data Info:\")\n",
    "    trip_data_processor.get_info()\n",
    "\n",
    "    print(\"\\nFetching weather data from API...\")\n",
    "    weather_df = weather_data_processor.fetch_weather_data(dataset_id, station_id, start_date, end_date, units)\n",
    "\n",
    "    print(\"Sample data from Weather Dataset\\n\", weather_df.head())\n",
    "    print(\"Weather Data Columns:\", weather_df.columns)\n",
    "\n",
    "    print(\"Pivoting weather data...\")\n",
    "    weather_pivoted_df = weather_data_processor.pivot_weather_data(weather_df)\n",
    "\n",
    "    print(\"Pivoted Weather Data:\")\n",
    "    print(weather_pivoted_df.head())\n",
    "\n",
    "    print(\"Deriving date column for weather data...\")\n",
    "    #weather_pivoted_df = utils_instance.derive_date_columns(weather_pivoted_df, 'date', 'weather_date')\n",
    "\n",
    "    weather_pivoted_df = utils.derive_date_columns(weather_pivoted_df, 'date', 'weather_date')\n",
    "\n",
    "    print(\"Deriving date column from trip data...\")\n",
    "    trip_df = utils.derive_date_columns(trip_df, 'tpep_pickup_datetime', 'pickup_date')\n",
    "    trip_df = utils.derive_date_columns(trip_df, 'tpep_dropoff_datetime', 'dropoff_date')\n",
    "\n",
    "    print(\"Sample data after deriving date column:\")\n",
    "    print(weather_pivoted_df.head())\n",
    "\n",
    "    print(\"Sample data after deriving date column from trip data:\")\n",
    "    print(trip_df.head())\n",
    "\n",
    "    # dqc = DQChecker(trip_df)\n",
    "    # print(\"\\nRunning Data Quality Checks on Trip Data:\")\n",
    "    # dqc_results = dqc.run_all_checks()\n",
    "    # print(\"Null Values:\\n\", dqc_results['nulls'])\n",
    "    # print(\"Duplicate Rows:\", dqc_results['duplicates'])\n",
    "    # print(\"Data Types:\\n\", dqc_results['data_types'])   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a741034",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "duplicate_check(\n",
    "    df_trip_orig,\n",
    "    session,\n",
    "    \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_DUPLICATES\"\n",
    ")\n",
    "\n",
    "df_trip_orig = session.table(\"INBOUND_INTEGRATION.LANDING_TRIP.YELLOW_TRIP_RECORDS\")\n",
    "df_weather = session.table(\"INBOUND_INTEGRATION.LANDING_WEATHER.NYC_WEATHER\")\n",
    "#print(\"weather data\")\n",
    "#df_weather.show()\n",
    "\n",
    "# Call null_check as usual\n",
    "null_count = null_check(df_trip_orig, session, \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_NULL_RECORDS\")\n",
    "#null_count = null_check(df_weather, session, \"INBOUND_INTEGRATION.DQ_WEATHER.WEATHER_NULL_RECORDS\")\n",
    "\n",
    "if null_count == 0:\n",
    "    print(\"Proceeding to next DQ checks.\")\n",
    "else:\n",
    "    print(f\"{null_count} rows with null/empty values found. Review DQ_TRIPDATA_NULLS table.\")\n",
    "\n",
    "trip_int_columns = [ '\"VendorID\"',\n",
    "    '\"PULocationID\"', '\"DOLocationID\"'\n",
    "]\n",
    "\n",
    "trip_ts_columns = [\n",
    "    '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"'\n",
    "]\n",
    "\n",
    "#df_junk = junk_value_check(df_trip_orig, trip_int_columns)\n",
    "df_junk_t = junk_value_check(df_trip_orig, trip_ts_columns, type=\"timestamp_type\")\n",
    "df_junk_i = junk_value_check(df_trip_orig, trip_int_columns, type=\"integer_type\")\n",
    "df_junk_t.show()\n",
    "df_junk_i.show()\n",
    "print(f\"Junk records found: {df_junk_t.count() + df_junk_t.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69291530",
   "metadata": {},
   "source": [
    "## Above logic to be discarded - pandas based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38eb42",
   "metadata": {},
   "source": [
    "# Landing population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a3067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snowflake-snowpark-python\n",
      "  Downloading snowflake_snowpark_python-1.35.0-py3-none-any.whl.metadata (150 kB)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-snowpark-python) (65.5.0)\n",
      "Collecting wheel (from snowflake-snowpark-python)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting snowflake-connector-python<4.0.0,>=3.14.0 (from snowflake-snowpark-python)\n",
      "  Downloading snowflake_connector_python-3.16.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-snowpark-python) (4.13.2)\n",
      "Collecting pyyaml (from snowflake-snowpark-python)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting cloudpickle!=2.1.0,!=2.2.0,<=3.0.0,>=1.6.0 (from snowflake-snowpark-python)\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf<6,>=3.20 (from snowflake-snowpark-python)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: python-dateutil in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-snowpark-python) (2.9.0.post0)\n",
      "Collecting tzlocal (from snowflake-snowpark-python)\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Downloading boto3-1.40.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Downloading botocore-1.40.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting cffi<2.0.0,>=1.9 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Downloading cffi-1.17.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting cryptography>=3.1.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached cryptography-45.0.5-cp37-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting pyOpenSSL<26.0.0,>=22.0.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached pyopenssl-25.1.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pyjwt<3.0.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pytz in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.32.3)\n",
      "Requirement already satisfied: packaging in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2025.1.31)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.18.0)\n",
      "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (4.3.7)\n",
      "Collecting tomlkit (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from python-dateutil->snowflake-snowpark-python) (1.17.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from botocore>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.4.0)\n",
      "Collecting pycparser (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading snowflake_snowpark_python-1.35.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Downloading snowflake_connector_python-3.16.0-cp310-cp310-macosx_11_0_arm64.whl (993 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.1/993.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Downloading boto3-1.40.2-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.2-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.17.1-cp310-cp310-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached cryptography-45.0.5-cp37-abi3-macosx_10_9_universal2.whl (7.0 MB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached pyopenssl-25.1.0-py3-none-any.whl (56 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, asn1crypto, wheel, tzlocal, tomlkit, pyyaml, pyjwt, pycparser, protobuf, jmespath, cloudpickle, cffi, botocore, s3transfer, cryptography, pyOpenSSL, boto3, snowflake-connector-python, snowflake-snowpark-python\n",
      "Successfully installed asn1crypto-1.5.1 boto3-1.40.2 botocore-1.40.2 cffi-1.17.1 cloudpickle-3.0.0 cryptography-45.0.5 jmespath-1.0.1 protobuf-5.29.5 pyOpenSSL-25.1.0 pycparser-2.22 pyjwt-2.10.1 pyyaml-6.0.2 s3transfer-0.13.1 snowflake-connector-python-3.16.0 snowflake-snowpark-python-1.35.0 sortedcontainers-2.4.0 tomlkit-0.13.3 tzlocal-5.3.1 wheel-0.45.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install snowflake-snowpark-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2671a263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\",override=True)\n",
    "print(os.getcwd())\n",
    "\n",
    "print(os.environ[\"SNOWFLAKE_ACCOUNT\"])\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # try without path first\n",
    "\n",
    "# for k in [\"SNOWFLAKE_ACCOUNT\", \"SNOWFLAKE_USER\", \"SNOWFLAKE_PASSWORD\", \"SNOWFLAKE_ROLE\"]:\n",
    "#     print(f\"{k} = {os.getenv(k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc433940",
   "metadata": {},
   "source": [
    "# Weather data schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark.types import StructType, StructField, StringType\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"date\", StringType()),\n",
    "#     StructField(\"datatype\", StringType()),\n",
    "#     StructField(\"station\", StringType()),\n",
    "#     StructField(\"attributes\", StringType()),\n",
    "#     StructField(\"value\", StringType())  # stored as string even if numeric\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5ca7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather API data ingested directly into Snowflake landing table using Snowpark without pandas.\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "# Snowflake connection\n",
    "connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "   # \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"INBOUND_INTEGRATION\",\n",
    "    \"schema\": \"LANDING_WEATHER\"\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "# Fetch weather data from API\n",
    "TOKEN = os.environ[\"NCEI_TOKEN\"]\n",
    "endpoint = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "headers = {\"token\": TOKEN}\n",
    "params = {\n",
    "    \"datasetid\": \"GHCND\",\n",
    "    \"stationid\": \"GHCND:USW00094728\",\n",
    "    \"startdate\": \"2025-05-01\",\n",
    "    \"enddate\": \"2025-05-31\",\n",
    "    \"units\": \"metric\",\n",
    "    \"limit\": 1000\n",
    "}\n",
    "\n",
    "response = requests.get(endpoint, headers=headers, params=params)\n",
    "data = response.json()[\"results\"]  # This is already a list of dicts\n",
    "\n",
    "\n",
    "\n",
    "#for row in data[:5]:\n",
    "#    print(row)\n",
    "\n",
    "# Create Snowpark DataFrame directly\n",
    "df_snowpark = session.create_dataframe(data,schema=schema)\n",
    "\n",
    "# Write to Snowflake landing table\n",
    "df_snowpark.write.mode(\"overwrite\").save_as_table(\"INBOUND_INTEGRATION.LANDING_WEATHER.NYC_WEATHER\")\n",
    "\n",
    "print(\"Weather API data ingested directly into Snowflake landing table using Snowpark without pandas.\")\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450513e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file ingested into YELLOW_TRIP_RECORDS successfully.\n",
      "---------------------------------------------------------------------------\n",
      "|\"LOCATIONID\"  |\"BOROUGH\"      |\"ZONE\"                   |\"SERVICE_ZONE\"  |\n",
      "---------------------------------------------------------------------------\n",
      "|1             |EWR            |Newark Airport           |EWR             |\n",
      "|2             |Queens         |Jamaica Bay              |Boro Zone       |\n",
      "|3             |Bronx          |Allerton/Pelham Gardens  |Boro Zone       |\n",
      "|4             |Manhattan      |Alphabet City            |Yellow Zone     |\n",
      "|5             |Staten Island  |Arden Heights            |Boro Zone       |\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.functions import col \n",
    "\n",
    "# Session connection\n",
    "connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "   # \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"INBOUND_INTEGRATION\",\n",
    "    \"schema\": \"LANDING_TRIP\"\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "#  Read Parquet file from stage\n",
    "#df_trip = session.read.parquet(\"@LANDING_TRIP_STAGE/yellow_tripdata_2025-05.parquet\")\n",
    "\n",
    "#df_trip.show(5)\n",
    "\n",
    "#  Write to your landing table (overwrite or append)\n",
    "#df_trip.write.mode(\"append\").save_as_table(\"YELLOW_TRIP_RECORDS\")\n",
    "\n",
    "print(\"Parquet file ingested into YELLOW_TRIP_RECORDS successfully.\")\n",
    "\n",
    "df_lookup = session.read.options({\n",
    "    \"FIELD_OPTIONALLY_ENCLOSED_BY\": '\"',\n",
    "    \"SKIP_HEADER\": 0\n",
    "}).csv(\"@landing_trip_stage/taxi_zone_lookup.csv.gz\")\n",
    "\n",
    "lookup_headers = df_lookup.first()\n",
    "#lookup_schema = StructType([StructField(cols.strip(), StringType()) for cols in lookup_headers])\n",
    "\n",
    "df_lookup = df_lookup.to_df(*lookup_headers) #creates new dataframe with columns as provided list\n",
    "\n",
    "\n",
    "# Using filter to remove the header row based on the first column value\n",
    "df_lookup = df_lookup.filter(col(lookup_headers[0]) != lookup_headers[0])\n",
    "\n",
    "df_lookup.show(5)\n",
    "df_lookup.write.mode(\"overwrite\").save_as_table(\"TAXI_ZONE_LOOKUP\")\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9f4c8",
   "metadata": {},
   "source": [
    "# DQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key_columns_needed = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID']\n",
    "t_data_key_columns = [f'\"{col}\"' for col in key_columns_needed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a37d573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_columns_needed = ['DATE', 'DATATYPE']\n",
    "t_data_key_columns = [f'\"{col}\"' for col in key_columns_needed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be8ba6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, lit, trim\n",
    "\n",
    "def build_null_condition(key_columns):\n",
    "    \"\"\"\n",
    "    Builds a condition to check for null or empty values in key columns.\n",
    "    \"\"\"\n",
    "    null_condition = None\n",
    "    for column in key_columns:\n",
    "        #clean_col = column.strip('\"')\n",
    "        condition = col(column).is_null() |  (trim(col(column)) == lit(\"\"))\n",
    "        null_condition = condition if null_condition is None else null_condition | condition\n",
    "    return null_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, lit\n",
    "# Session connection\n",
    "connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "   # \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"INBOUND_INTEGRATION\",\n",
    "    \"schema\": \"LANDING_TRIP\"\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "def null_check(df, session, dq_table_name):\n",
    "    \"\"\"\n",
    "    Checks for null/empty values in critical composite key columns\n",
    "    and stores violating rows in a DQ table.\n",
    "    \"\"\"\n",
    "    cond = build_null_condition(t_data_key_columns)\n",
    "\n",
    "    print(cond)\n",
    "\n",
    "    print (\"Invalid rows with null/empty values in key columns\")\n",
    "    df_invalid = df.filter(cond)\n",
    "    #print(df_invalid.schema.names)\n",
    "    df_invalid.show()\n",
    "    df_valid = df.filter(~cond)\n",
    "    print(\"Valid rows with no null/empty values in key columns\")\n",
    "    df_valid.show()\n",
    "\n",
    "    print(\"Total:\", df.count())\n",
    "    print(\"Invalid (nulls):\", df.filter(cond).count())\n",
    "    print(\"Valid:\", df.filter(~cond).count())\n",
    "    #print(df_invalid.count())\n",
    "    if df_invalid.count() > 0:\n",
    "        print(f\"Found {df_invalid.count()} rows with null/empty values in key columns.\")\n",
    "        df_invalid.write.mode(\"overwrite\").save_as_table(dq_table_name)\n",
    "\n",
    "    else:\n",
    "        print(\"No null/empty values found in key columns.\")\n",
    "    return df_invalid.count()\n",
    "\n",
    "df_trip_orig = session.table(\"INBOUND_INTEGRATION.LANDING_TRIP.YELLOW_TRIP_RECORDS\")\n",
    "df_weather = session.table(\"INBOUND_INTEGRATION.LANDING_WEATHER.NYC_WEATHER\")\n",
    "print(\"weather data\")\n",
    "#df_weather.show()\n",
    "# Strip quotes immediately:\n",
    "#cleaned_columns = [col_name.strip('\"') for col_name in df_trip_orig.schema.names]\n",
    "#print(cleaned_columns)\n",
    "\n",
    "\n",
    "# Call null_check as usual\n",
    "null_count = null_check(df_trip_orig, session, \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_NULL_RECORDS\")\n",
    "#null_count = null_check(df_weather, session, \"INBOUND_INTEGRATION.DQ_WEATHER.WEATHER_NULL_RECORDS\")\n",
    "\n",
    "\n",
    "if null_count == 0:\n",
    "    print(\"✅ Proceeding to next DQ checks.\")\n",
    "else:\n",
    "    print(f\"⚠️ {null_count} rows with null/empty values found. Review DQ_TRIPDATA_NULLS table.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, count, lit, row_number\n",
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "\n",
    "def duplicate_check(df, session, dq_table_name):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows based on composite key columns\n",
    "    and stores violating rows in a DQ table.\n",
    "    \"\"\"\n",
    "\n",
    "    # introduce row number to append only the first occurence of the duplicated records to clean and the rest to duplicates tables\n",
    "    df_full_dedup = df.with_column('rn', \n",
    "        (row_number().over(Window.partition_by([col(c) for c in t_data_key_columns]).order_by([col(c) for c in t_data_key_columns]))))\n",
    "    df_full_dedup.show()\n",
    "    df_full_dedup_clean = df_full_dedup.filter(col('rn') == 1)\n",
    "    df_full_dedup = df_full_dedup.filter(col('rn') != 1).drop('rn')\n",
    "    # To be passed to junk check \n",
    "    #df_clean_final = df_clean.union(df_full_dedup_clean)\n",
    "\n",
    "    clean_count = df_full_dedup_clean.count()\n",
    "    print (f\"{clean_count} clean records\")\n",
    "\n",
    "    dupe_count = df_full_dedup.count()\n",
    "    \n",
    "    #df_duplicates.show()\n",
    "\n",
    "    if dupe_count > 0:\n",
    "        print(f\"{dupe_count} duplicate rows found. Recording to {dq_table_name}\")\n",
    "        df_full_dedup.write.mode(\"overwrite\").save_as_table(dq_table_name)\n",
    "    else:\n",
    "        print(\"Duplicate check passed: No duplicate found\")\n",
    "\n",
    "    return dupe_count\n",
    "\n",
    "duplicate_check(\n",
    "    df_trip_orig,\n",
    "    session,\n",
    "    \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_DUPLICATES\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69850\n"
     ]
    }
   ],
   "source": [
    "#ignore this block\n",
    "\n",
    "from snowflake.snowpark.functions import col, count, lit\n",
    "\n",
    "key_columns_needed = ['\"VendorID\"', '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"', '\"PULocationID\"', '\"DOLocationID\"']\n",
    "#key_columns_needed = ['\"date\"', '\"datatype\"']\n",
    "t_data_key_columns = key_columns_needed\n",
    "\n",
    "\n",
    "df_duplicates = (\n",
    "    df_trip_orig.group_by([col(c) for c in t_data_key_columns])\n",
    "    .agg(count(lit(1)).alias(\"record_count\"))\n",
    "    .filter(col(\"record_count\") > 1)\n",
    ")\n",
    "\n",
    "print(df_duplicates.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7095151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, trim, try_cast\n",
    "from snowflake.snowpark.types import IntegerType,TimestampType\n",
    "\n",
    "trip_int_columns = [ '\"VendorID\"',\n",
    "    '\"PULocationID\"', '\"DOLocationID\"'\n",
    "]\n",
    "\n",
    "trip_ts_columns = [\n",
    "    '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"'\n",
    "]\n",
    "\n",
    "\n",
    "def junk_value_check(df, numeric_columns):\n",
    "    \"\"\"\n",
    "    Detects non-numeric (junk) values in string columns that are expected to be numeric,\n",
    "    using regex and ignoring null/empty strings.\n",
    "    \"\"\"\n",
    "    junk_condition = None\n",
    "    for column in numeric_columns:\n",
    "        trimmed_col = trim(col(column))\n",
    "        # Matches anything NOT a valid number: not digits, not optional dot, not optional minus\n",
    "        #condition = try_cast(trimmed_col, IntegerType() ).is_null()\n",
    "        condition = try_cast(trimmed_col, TimestampType() ).is_null()\n",
    "        junk_condition = condition if junk_condition is None else (junk_condition | condition)\n",
    "    \n",
    "    return df.filter(junk_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import IntegerType, TimestampType\n",
    "\n",
    "# Define expected types per column - later to belong to config\n",
    "expected_types = {\n",
    "    '\"PULocationID\"': IntegerType(),\n",
    "    '\"DOLocationID\"': IntegerType(),\n",
    "    '\"tpep_pickup_datetime\"': TimestampType(),\n",
    "    '\"tpep_dropoff_datetime\"': TimestampType()\n",
    "}\n",
    "\n",
    "#df_junk = junk_value_check(df_trip_orig, trip_int_columns)\n",
    "df_junk = junk_value_check(df_trip_orig, trip_ts_columns)\n",
    "df_junk.show()\n",
    "print(f\"Junk records found: {df_junk.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30d604",
   "metadata": {},
   "source": [
    "### Working DQ timestamp check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b18f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, trim, try_cast\n",
    "\n",
    "def junk_value_check(df, columns, type, min_valid_epoch=1600000000000):\n",
    "    \"\"\"\n",
    "    Flags rows where timestamp fields are present but invalid:\n",
    "    - Either not castable to integer\n",
    "    - Or below a minimum valid epoch threshold\n",
    "    \"\"\"\n",
    "    junk_condition = None\n",
    "\n",
    "    for column in columns:\n",
    "        trimmed_col = trim(col(column))\n",
    "        casted_col = try_cast(trimmed_col, IntegerType())\n",
    "        if type == \"timestamp_type\":\n",
    "            condition = ((casted_col.is_null()) | (casted_col < min_valid_epoch))\n",
    "        else :\n",
    "              condition = (casted_col.is_null())\n",
    "\n",
    "        junk_condition = condition if junk_condition is None else (junk_condition | condition)\n",
    "\n",
    "    return df.filter(junk_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import IntegerType, TimestampType\n",
    "\n",
    "# Define expected types per column - later to belong to config\n",
    "expected_types = {\n",
    "    '\"PULocationID\"': IntegerType(),\n",
    "    '\"DOLocationID\"': IntegerType(),\n",
    "    '\"tpep_pickup_datetime\"': TimestampType(),\n",
    "    '\"tpep_dropoff_datetime\"': TimestampType()\n",
    "}\n",
    "\n",
    "trip_int_columns = [ '\"VendorID\"',\n",
    "    '\"PULocationID\"', '\"DOLocationID\"'\n",
    "]\n",
    "\n",
    "trip_ts_columns = [\n",
    "    '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"'\n",
    "]\n",
    "\n",
    "#df_junk = junk_value_check(df_trip_orig, trip_int_columns)\n",
    "df_junk_t = junk_value_check(df_trip_orig, trip_ts_columns, type=\"timestamp_type\")\n",
    "df_junk_i = junk_value_check(df_trip_orig, trip_int_columns, type=\"integer_type\")\n",
    "df_junk_t.show()\n",
    "df_junk_i.show()\n",
    "print(f\"Junk records found: {df_junk_t.count() + df_junk_t.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27cd4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake connection\n",
    "connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "   # \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"INBOUND_INTEGRATION\",\n",
    "    \"schema\": \"LANDING_WEATHER\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d9e0d6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, lit, trim, row_number, try_cast\n",
    "from snowflake.snowpark.window import Window\n",
    "from snowflake.snowpark.types import IntegerType,TimestampType\n",
    "\n",
    "class DQCheck :\n",
    "\n",
    "    def __init__(self,session,key_columns, dq_table_name ):\n",
    "        self.session = session\n",
    "        self.key_columns = key_columns\n",
    "        #self.t_key_columns = t_key_columns\n",
    "        #self.w_key_columns = w_key_columns\n",
    "        self.dq_table_name = dq_table_name\n",
    "\n",
    "    def build_null_condition(self):\n",
    "        \"\"\"\n",
    "        Builds a condition to check for null or empty values in key columns.\n",
    "        \"\"\"\n",
    "        null_condition = None\n",
    "        for column in self.key_columns:\n",
    "            #clean_col = column.strip('\"')\n",
    "            condition = col(column).is_null() |  (trim(col(column)) == lit(\"\"))\n",
    "            null_condition = condition if null_condition is None else null_condition | condition\n",
    "        return null_condition\n",
    "    \n",
    "\n",
    "    def null_check(self,df,session,dq_table_name):\n",
    "        \"\"\"\n",
    "        Checks for null/empty values in critical composite key columns\n",
    "        and stores violating rows in a DQ table.\n",
    "        \"\"\"\n",
    "        cond = self.build_null_condition()\n",
    "\n",
    "        print(cond)\n",
    "\n",
    "        print (\"Invalid rows with null/empty values in key columns\")\n",
    "        df_invalid = df.filter(cond)\n",
    "        #print(df_invalid.schema.names)\n",
    "        df_invalid.show()\n",
    "        df_valid = df.filter(~cond)\n",
    "        print(\"Valid rows with no null/empty values in key columns\")\n",
    "        df_valid.show()\n",
    "\n",
    "        print(\"Total:\", df.count())\n",
    "        print(\"Invalid (nulls):\", df.filter(cond).count())\n",
    "        print(\"Valid:\", df.filter(~cond).count())\n",
    "        #print(df_invalid.count())\n",
    "        if df_invalid.count() > 0:\n",
    "            print(f\"Found {df_invalid.count()} rows with null/empty values in key columns.\")\n",
    "            df_invalid.write.mode(\"overwrite\").save_as_table(dq_table_name)\n",
    "\n",
    "        else:\n",
    "            print(\"No null/empty values found in key columns.\")\n",
    "        return df_invalid.count(), df_valid\n",
    "    \n",
    "    def duplicate_check(self, df, session, dq_table_name):\n",
    "        \"\"\"\n",
    "        Checks for duplicate rows based on composite key columns\n",
    "        and stores violating rows in a DQ table.\n",
    "        \"\"\"\n",
    "\n",
    "        # introduce row number to append only the first occurence of the duplicated records to clean and the rest to duplicates tables\n",
    "        df_full_dedup = df.with_column('rn', \n",
    "            (row_number().over(Window.partition_by([col(c) for c in self.key_columns]).order_by([col(c) for c in self.key_columns]))))\n",
    "        #df_full_dedup.show()\n",
    "        df_full_dedup_clean = df_full_dedup.filter(col('rn') == 1)\n",
    "        df_full_dedup_clean = df_full_dedup_clean.drop('rn')\n",
    "        df_full_dedup = df_full_dedup.filter(col('rn') != 1).drop('rn')\n",
    "        # To be passed to junk check \n",
    "        #df_clean_final = df_clean.union(df_full_dedup_clean)\n",
    "        df_full_dedup.show()\n",
    "\n",
    "        clean_count = df_full_dedup_clean.count()\n",
    "        print (f\"{clean_count} clean records\")\n",
    "\n",
    "        dupe_count = df_full_dedup.count()\n",
    "        \n",
    "        if dupe_count > 0:\n",
    "            print(f\"{dupe_count} duplicate rows found. Recording to {dq_table_name}\")\n",
    "            df_full_dedup.write.mode(\"overwrite\").save_as_table(dq_table_name)\n",
    "        else:\n",
    "            print(\"Duplicate check passed: No duplicate found\")\n",
    "\n",
    "        return dupe_count, df_full_dedup_clean\n",
    "\n",
    "    def junk_value_check(self, df, columns, dtype, min_valid_epoch=1600000000000):\n",
    "        \"\"\"\n",
    "        Flags rows where timestamp fields are present but invalid:\n",
    "        - Either not castable to integer\n",
    "        - Or below a minimum valid epoch threshold\n",
    "        \"\"\"\n",
    "        junk_condition = None\n",
    "\n",
    "        for column in columns:\n",
    "            trimmed_col = trim(col(column))\n",
    "            casted_col = try_cast(trimmed_col, IntegerType())\n",
    "            if dtype == \"timestamp_type\":\n",
    "                condition = ((casted_col.is_null()) | (casted_col < min_valid_epoch))\n",
    "            else :\n",
    "                condition = (casted_col.is_null())\n",
    "\n",
    "            junk_condition = condition if junk_condition is None else (junk_condition | condition)\n",
    "\n",
    "        return df.filter(junk_condition),df.filter(~junk_condition)\n",
    "    \n",
    "    def weather_junk_val_check (self,df,columns,dtype,weather_data_types):\n",
    "        junk_condition = None\n",
    "\n",
    "        for column in columns:\n",
    "            trimmed_col = trim(col(column))\n",
    "            \n",
    "            if dtype == \"timestamp_type\":\n",
    "                casted_col = try_cast(trimmed_col, TimestampType())\n",
    "                condition = (casted_col.is_null())\n",
    "                             \n",
    "            elif dtype == \"datatype_check\":\n",
    "                condition = ~trimmed_col.isin(weather_data_types)\n",
    "                \n",
    "            else:\n",
    "                print (\"Unrecognized dtype\")\n",
    "\n",
    "            junk_condition = condition if junk_condition is None else (junk_condition | condition)\n",
    "\n",
    "        return df.filter(junk_condition),df.filter(~junk_condition)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "622792c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "trip_key_cols = ['\"VendorID\"', '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"', '\"PULocationID\"', '\"DOLocationID\"']\n",
    "weather_key_cols = ['DATE', 'DATATYPE']\n",
    "weather_data_types = [\"AWND\", \"WT01\", \"WSF5\", \"WSF2\", \"WDF5\", \"WDF2\", \"TMIN\", \"TMAX\", \"SNWD\", \"PRCP\", \"WT08\", \"SNOW\", \"WT03\", \"WT02\"]\n",
    "\n",
    "trip_ts_columns = ['\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"']\n",
    "trip_int_columns = ['\"VendorID\"', '\"PULocationID\"', '\"DOLocationID\"']\n",
    "\n",
    "weather_ts_columns = ['date']\n",
    "weather_data_columns = ['datatype']\n",
    "\n",
    "\n",
    "dq_table_name=\"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_NULL_RECORDS\"\n",
    "invalid_trip_data = \"INBOUND_INTEGRATION.DQ_TRIP.INVALID_TRIP_DATA\"\n",
    "valid_trip_data=\"INBOUND_INTEGRATION.SDS_TRIP.TRIP_DATA_VALIDATED\"\n",
    "trip_duplicates = \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_DUPLICATES\"\n",
    "raw_trip_data = \"INBOUND_INTEGRATION.LANDING_TRIP.YELLOW_TRIP_RECORDS\"\n",
    "\n",
    "weather_null = \"INBOUND_INTEGRATION.DQ_WEATHER.WEATHER_NULL_RECORDS\"\n",
    "weather_duplicates = \"INBOUND_INTEGRATION.DQ_WEATHER.WEATHER_DATA_DUPLICATES\"\n",
    "valid_weather_data = \"INBOUND_INTEGRATION.SDS_WEATHER.WEATHER_DATA_VALIDATED\"\n",
    "invalid_weather_data = \"INBOUND_INTEGRATION.DQ_WEATHER.INVALID_WEATHER_DATA\"\n",
    "raw_weather_data=\"INBOUND_INTEGRATION.LANDING_WEATHER.NYC_WEATHER\"\n",
    "\n",
    "zone_lookup = \"INBOUND_INTEGRATION.LANDING_TRIP.TAXI_ZONE_LOOKUP\"\n",
    "final_table = \"OUTBOUND_INTEGRATION.TRIP_ANALYTICS.TRIP_ANALYTICS\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f4d53565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col,min,to_date\n",
    "\n",
    "valid_weather_data = \"INBOUND_INTEGRATION.SDS_WEATHER.WEATHER_DATA_VALIDATED\"\n",
    "\n",
    "def pivot_weather_table():\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    df = session.table(valid_weather_data)\n",
    "    \n",
    "    datatypes = [row[0] for row in df.select (\"datatype\").distinct().collect()]\n",
    "    #print (datatypes[0])\n",
    "    #pivoted_df = df.pivot(datatypes).on(\"value\").group_by(\"date\").agg()\n",
    "\n",
    "    pivoted_df = (\n",
    "        df.group_by(\"DATE\")                       # <- group first\n",
    "        .pivot(\"datatype\",datatypes)              # <- then pivot on DATATYPE values\n",
    "        .agg(min(col(\"VALUE\")))             # <- single aggregate is fine (and typical)\n",
    "    )\n",
    "\n",
    "\n",
    "    pivoted_df=pivoted_df.with_column('\"ODate\"',to_date(col(\"DATE\"))\n",
    "                                    ).with_column_renamed(\"'TMIN'\",'\"Tmin\"'\n",
    "                                    ).with_column_renamed(\"'TMAX'\",'\"Tmax\"'\n",
    "                                    ).with_column_renamed(\"'PRCP'\",'\"Prcp\"'\n",
    "                                    ).with_column_renamed(\"'SNOW'\",'\"Snow\"'\n",
    "                                    ).with_column_renamed(\"'SNWD'\",'\"Snwd\"'\n",
    "                                    ).with_column_renamed(\"'AWND'\",'\"Awnd\"'\n",
    "                                    ).with_column_renamed(\"'WSF5'\",'\"Wsf5\"'\n",
    "                                    ).with_column_renamed(\"'WDF5'\",'\"Wdf5\"'\n",
    "                                    ).with_column_renamed(\"'WSF2'\",'\"Wsf2\"'\n",
    "                                    ).with_column_renamed(\"'WDF2'\",'\"Wdf2\"')\n",
    "\n",
    "    return pivoted_df.select('\"ODate\"','\"Tmin\"','\"Tmax\"','\"Prcp\"','\"Snow\"','\"Snwd\"','\"Awnd\"','\"Wsf5\"','\"Wdf5\"','\"Wsf2\"','\"Wdf2\"')#,\"'WT01'\",\"'WT02'\",\"'WT03'\",\"'WT08'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "66d8ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, to_timestamp,concat_ws\n",
    "\n",
    "valid_trip_data=\"INBOUND_INTEGRATION.SDS_TRIP.TRIP_DATA_VALIDATED\"\n",
    "\n",
    "def trip_records_transformer():\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    df = session.table(valid_trip_data)\n",
    "    #df.show()\n",
    "    transformed_df=(df.with_column('\"PickupTime\"',to_timestamp('\"tpep_pickup_datetime\"'))\n",
    "        .with_column('\"DropoffTime\"',to_timestamp('\"tpep_dropoff_datetime\"'))\n",
    "        .with_column('\"RideDate\"', to_date('\"tpep_pickup_datetime\"'))\n",
    "        .with_column_renamed('\"passenger_count\"', '\"PassengerCount\"')\n",
    "        .with_column_renamed('\"trip_distance\"', '\"TripDistance\"')\n",
    "        .with_column_renamed('\"fare_amount\"', '\"FareAmount\"')\n",
    "        .with_column_renamed('\"tip_amount\"', '\"TipAmount\"')\n",
    "        .with_column_renamed('\"tolls_amount\"', '\"TollsAmount\"')\n",
    "        .with_column_renamed('\"total_amount\"', '\"TotalAmount\"')\n",
    "        .with_column_renamed('\"Airport_fee\"', '\"AirportFee\"')\n",
    "        .with_column_renamed('\"cbd_congestion_fee\"', '\"CbdCongestionFee\"')\n",
    "        )\n",
    "    \n",
    "    return transformed_df.select('\"VendorID\"','\"RideDate\"','\"PickupTime\"','\"DropoffTime\"','\"TripDistance\"','\"PULocationID\"','\"DOLocationID\"','\"FareAmount\"','\"TipAmount\"','\"TollsAmount\"','\"TotalAmount\"') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8f29343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column[\"VendorID\" IS NULL OR trim = LITERAL OR \"tpep_pickup_datetime\" IS NULL OR trim = LITERAL OR \"tpep_dropoff_datetime\" IS NULL OR trim = LITERAL OR \"PULocationID\" IS NULL OR trim = LITERAL OR \"DOLocationID\" IS NULL OR trim = LITERAL]\n",
      "Invalid rows with null/empty values in key columns\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"VendorID\"  |\"tpep_pickup_datetime\"  |\"tpep_dropoff_datetime\"  |\"passenger_count\"  |\"trip_distance\"  |\"RatecodeID\"  |\"store_and_fwd_flag\"  |\"PULocationID\"  |\"DOLocationID\"  |\"payment_type\"  |\"fare_amount\"  |\"extra\"  |\"mta_tax\"  |\"tip_amount\"  |\"tolls_amount\"  |\"improvement_surcharge\"  |\"total_amount\"  |\"congestion_surcharge\"  |\"Airport_fee\"  |\"cbd_congestion_fee\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|            |                        |                         |                   |                 |              |                      |                |                |                |               |         |           |              |                |                         |                |                        |               |                      |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Valid rows with no null/empty values in key columns\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"VendorID\"  |\"tpep_pickup_datetime\"  |\"tpep_dropoff_datetime\"  |\"passenger_count\"  |\"trip_distance\"  |\"RatecodeID\"  |\"store_and_fwd_flag\"  |\"PULocationID\"  |\"DOLocationID\"  |\"payment_type\"  |\"fare_amount\"  |\"extra\"  |\"mta_tax\"  |\"tip_amount\"  |\"tolls_amount\"  |\"improvement_surcharge\"  |\"total_amount\"  |\"congestion_surcharge\"  |\"Airport_fee\"  |\"cbd_congestion_fee\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1           |1746058026000000        |1746059055000000         |1                  |3.7              |1             |N                     |140             |202             |1               |18.4           |4.25     |0.5        |4.85          |0               |1                        |29              |2.5                     |0              |0.75                  |\n",
      "|2           |1746058064000000        |1746058467000000         |1                  |1.03             |1             |N                     |234             |161             |1               |8.6            |1        |0.5        |4.3           |0               |1                        |18.65           |2.5                     |0              |0.75                  |\n",
      "|2           |1746058556000000        |1746059033000000         |1                  |1.57             |1             |N                     |161             |234             |2               |10             |1        |0.5        |0             |0               |1                        |15.75           |2.5                     |0              |0.75                  |\n",
      "|2           |1746057609000000        |1746059129000000         |1                  |9.48             |1             |N                     |138             |90              |1               |40.8           |6        |0.5        |11.7          |6.94            |1                        |71.94           |2.5                     |1.75           |0.75                  |\n",
      "|2           |1746060307000000        |1746060765000000         |1                  |1.8              |1             |N                     |90              |231             |1               |10             |1        |0.5        |1.5           |0               |1                        |17.25           |2.5                     |0              |0.75                  |\n",
      "|2           |1746058164000000        |1746058924000000         |1                  |5.11             |1             |N                     |138             |226             |1               |22.6           |6        |0.5        |6.02          |0               |1                        |37.87           |0                       |1.75           |0                     |\n",
      "|1           |1746058694000000        |1746059258000000         |0                  |1.5              |1             |N                     |140             |263             |1               |11.4           |3.5      |0.5        |4.05          |0               |1                        |20.45           |2.5                     |0              |0                     |\n",
      "|2           |1746057034000000        |1746057366000000         |2                  |0.99             |1             |N                     |234             |79              |1               |7.9            |1        |0.5        |2.73          |0               |1                        |16.38           |2.5                     |0              |0.75                  |\n",
      "|2           |1746057885000000        |1746058063000000         |1                  |0.47             |1             |N                     |114             |144             |2               |5.1            |1        |0.5        |0             |0               |1                        |10.85           |2.5                     |0              |0.75                  |\n",
      "|7           |1746058951000000        |1746058951000000         |1                  |1.09             |1             |N                     |229             |43              |1               |8.6            |0        |0.5        |2.87          |0               |1                        |17.22           |2.5                     |0              |0.75                  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Total: 4591845\n",
      "Invalid (nulls): 0\n",
      "Valid: 4591845\n",
      "No null/empty values found in key columns.\n",
      "Proceeding to duplicate check\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"VendorID\"  |\"tpep_pickup_datetime\"  |\"tpep_dropoff_datetime\"  |\"passenger_count\"  |\"trip_distance\"  |\"RatecodeID\"  |\"store_and_fwd_flag\"  |\"PULocationID\"  |\"DOLocationID\"  |\"payment_type\"  |\"fare_amount\"  |\"extra\"  |\"mta_tax\"  |\"tip_amount\"  |\"tolls_amount\"  |\"improvement_surcharge\"  |\"total_amount\"  |\"congestion_surcharge\"  |\"Airport_fee\"  |\"cbd_congestion_fee\"  |\"RN\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1           |1748281597000000        |1748283449000000         |1                  |15.4             |1             |N                     |138             |13              |1               |57.6           |10       |0.5        |15.2          |6.94            |1                        |91.24           |2.5                     |1.75           |0.75                  |1     |\n",
      "|2           |1747905942000000        |1747907223000000         |1                  |3.19             |1             |N                     |238             |140             |1               |21.2           |0        |0.5        |3.75          |0               |1                        |28.95           |2.5                     |0              |0                     |1     |\n",
      "|2           |1746801744000000        |1746802389000000         |NULL               |3.49             |NULL          |NULL                  |42              |75              |0               |16.59          |0        |0.5        |0             |0               |1                        |18.09           |NULL                    |NULL           |0                     |1     |\n",
      "|2           |1747300182000000        |1747301760000000         |1                  |8.61             |1             |N                     |229             |138             |1               |38.7           |5        |0.5        |8             |0               |1                        |56.45           |2.5                     |0              |0.75                  |1     |\n",
      "|2           |1748195467000000        |1748196642000000         |NULL               |6.86             |NULL          |NULL                  |65              |260             |0               |25.92          |0        |0.5        |0             |0               |1                        |27.42           |NULL                    |NULL           |0                     |1     |\n",
      "|2           |1747926767000000        |1747927426000000         |1                  |1.48             |1             |N                     |234             |158             |1               |11.4           |0        |0.5        |2.42          |0               |1                        |18.57           |2.5                     |0              |0.75                  |1     |\n",
      "|2           |1746630870000000        |1746631801000000         |1                  |1.6              |1             |N                     |237             |164             |1               |14.9           |0        |0.5        |4             |0               |1                        |23.65           |2.5                     |0              |0.75                  |1     |\n",
      "|2           |1746382113000000        |1746385799000000         |1                  |21.43            |2             |N                     |132             |261             |1               |70             |0        |0.5        |15            |6.94            |1                        |98.44           |2.5                     |1.75           |0.75                  |1     |\n",
      "|2           |1746116877000000        |1746117174000000         |1                  |0.22             |1             |N                     |262             |262             |1               |5.8            |2.5      |0.5        |1             |0               |1                        |13.3            |2.5                     |0              |0                     |1     |\n",
      "|2           |1747553162000000        |1747554226000000         |1                  |10.89            |1             |N                     |233             |138             |1               |42.2           |5        |0.5        |11.78         |6.94            |1                        |70.67           |2.5                     |0              |0.75                  |1     |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "4521993 clean records\n",
      "69852 duplicate rows found. Recording to INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_NULL_RECORDS\n",
      "4521993 Total valid trip records written into INBOUND_INTEGRATION.SDS_TRIP.TRIP_DATA_VALIDATED table\n",
      "Column[\"DATE\" IS NULL OR trim = LITERAL OR \"DATATYPE\" IS NULL OR trim = LITERAL]\n",
      "Invalid rows with null/empty values in key columns\n",
      "------------------------------------------------------------\n",
      "|\"DATE\"  |\"DATATYPE\"  |\"STATION\"  |\"ATTRIBUTES\"  |\"VALUE\"  |\n",
      "------------------------------------------------------------\n",
      "|        |            |           |              |         |\n",
      "------------------------------------------------------------\n",
      "\n",
      "Valid rows with no null/empty values in key columns\n",
      "---------------------------------------------------------------------------------\n",
      "|\"DATE\"               |\"DATATYPE\"  |\"STATION\"          |\"ATTRIBUTES\"  |\"VALUE\"  |\n",
      "---------------------------------------------------------------------------------\n",
      "|2025-05-01T00:00:00  |AWND        |GHCND:USW00094728  |,,W,          |2.4      |\n",
      "|2025-05-31T00:00:00  |WT01        |GHCND:USW00094728  |,,W,          |1        |\n",
      "|2025-05-31T00:00:00  |WSF5        |GHCND:USW00094728  |,,W,          |16.1     |\n",
      "|2025-05-31T00:00:00  |WSF2        |GHCND:USW00094728  |,,W,          |8.1      |\n",
      "|2025-05-31T00:00:00  |WDF5        |GHCND:USW00094728  |,,W,          |300      |\n",
      "|2025-05-31T00:00:00  |WDF2        |GHCND:USW00094728  |,,W,          |290      |\n",
      "|2025-05-31T00:00:00  |TMIN        |GHCND:USW00094728  |,,W,2400      |11.7     |\n",
      "|2025-05-31T00:00:00  |TMAX        |GHCND:USW00094728  |,,W,2400      |19.4     |\n",
      "|2025-05-31T00:00:00  |SNWD        |GHCND:USW00094728  |,,W,2400      |0.0      |\n",
      "|2025-05-31T00:00:00  |SNOW        |GHCND:USW00094728  |,,W,          |0.0      |\n",
      "---------------------------------------------------------------------------------\n",
      "\n",
      "Total: 336\n",
      "Invalid (nulls): 0\n",
      "Valid: 336\n",
      "No null/empty values found in key columns.\n",
      "Proceeding to duplicate check\n",
      "----------------------------------------------------------------------------------------\n",
      "|\"DATE\"               |\"DATATYPE\"  |\"STATION\"          |\"ATTRIBUTES\"  |\"VALUE\"  |\"RN\"  |\n",
      "----------------------------------------------------------------------------------------\n",
      "|2025-05-05T00:00:00  |TMAX        |GHCND:USW00094728  |,,W,2400      |16.1     |1     |\n",
      "|2025-05-03T00:00:00  |WSF5        |GHCND:USW00094728  |,,W,          |10.7     |1     |\n",
      "|2025-05-19T00:00:00  |TMIN        |GHCND:USW00094728  |,,W,2400      |10.6     |1     |\n",
      "|2025-05-10T00:00:00  |WSF5        |GHCND:USW00094728  |,,W,          |14.8     |1     |\n",
      "|2025-05-15T00:00:00  |PRCP        |GHCND:USW00094728  |,,W,2400      |0.0      |1     |\n",
      "|2025-05-08T00:00:00  |WDF5        |GHCND:USW00094728  |,,W,          |360      |1     |\n",
      "|2025-05-23T00:00:00  |WSF2        |GHCND:USW00094728  |,,W,          |5.8      |1     |\n",
      "|2025-05-09T00:00:00  |WT01        |GHCND:USW00094728  |,,W,          |1        |1     |\n",
      "|2025-05-29T00:00:00  |WSF5        |GHCND:USW00094728  |,,W,          |8.9      |1     |\n",
      "|2025-05-17T00:00:00  |WSF2        |GHCND:USW00094728  |,,W,          |7.2      |1     |\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "336 clean records\n",
      "Duplicate check passed: No duplicate found\n",
      "336 Total valid weather data records written into INBOUND_INTEGRATION.SDS_WEATHER.WEATHER_DATA_VALIDATED table\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    df_trip_orig = session.table(raw_trip_data)\n",
    "    df_weather = session.table(raw_weather_data)\n",
    "    \n",
    "    weather_df = pivot_weather_table() \n",
    "    trip_df = trip_records_transformer()\n",
    "\n",
    "    dq = DQCheck(session, trip_key_cols, dq_table_name)\n",
    "\n",
    "    # Null Check\n",
    "    null_count, df_after_nulls = dq.null_check(df_trip_orig, session, dq_table_name)\n",
    "    if null_count == 0:\n",
    "        print(\"Proceeding to duplicate check\")\n",
    "    else:\n",
    "        print(\"Nulls found. Review before proceeding.\")\n",
    "\n",
    "    dupe_count, df_after_dupes = dq.duplicate_check(df_after_nulls,session,dq_table_name)\n",
    "\n",
    "    # Junk Check\n",
    "    df_junk_ts,df_clean_ts = dq.junk_value_check(df_after_dupes, trip_ts_columns, \"timestamp_type\")\n",
    "    df_junk_int,df_clean_int = dq.junk_value_check(df_clean_ts, trip_int_columns, \"integer_type\")\n",
    "\n",
    "    df_junk_int.write.mode(\"overwrite\").save_as_table(invalid_trip_data)\n",
    "    df_clean_int.write.mode(\"overwrite\").save_as_table(valid_trip_data)\n",
    "    print (f\"{df_clean_int.count()} Total valid trip records written into {valid_trip_data} table\")\n",
    "\n",
    "    ### Weather Data DQ Check ###\n",
    "    dq = DQCheck(session, weather_key_cols, dq_table_name)\n",
    "\n",
    "    # Null Check\n",
    "    null_count, df_after_nulls = dq.null_check(df_weather, session, weather_null)\n",
    "    if null_count == 0:\n",
    "        print(\"Proceeding to duplicate check\")\n",
    "    else:\n",
    "        print(\"Nulls found. Review before proceeding.\")\n",
    "\n",
    "    dupe_count, df_after_dupes = dq.duplicate_check(df_after_nulls, session, weather_null)\n",
    "\n",
    "    # Junk Check\n",
    "    df_junk_ts,df_clean_ts = dq.weather_junk_val_check(df_after_dupes, weather_ts_columns, \"timestamp_type\",weather_data_types )\n",
    "    df_junk_int,df_clean_int = dq.weather_junk_val_check(df_clean_ts, weather_data_columns, \"datatype_check\",weather_data_types)\n",
    "\n",
    "    df_junk_int.write.mode(\"overwrite\").save_as_table(invalid_weather_data)\n",
    "    df_clean_int.write.mode(\"overwrite\").save_as_table(valid_weather_data)\n",
    "    print (f\"{df_clean_int.count()} Total valid weather data records written into {valid_weather_data} table\")\n",
    "    \n",
    "    \n",
    "\n",
    "    z_lookup = session.table(zone_lookup)\n",
    "    z_pickup = z_lookup.select(\n",
    "        col(\"LOCATIONID\").alias(\"Pu_location_id\"),\n",
    "        col(\"BOROUGH\").alias(\"pu_borough\"),\n",
    "        col(\"zone\").alias(\"pu_zone\"))\n",
    "\n",
    "    z_drop = z_lookup.select(\n",
    "        col(\"LOCATIONID\").alias(\"do_location_id\"),\n",
    "        col(\"BOROUGH\").alias(\"do_borough\"),\n",
    "        col(\"zone\").alias(\"do_zone\"))\n",
    "\n",
    "\n",
    "    join_df = trip_df.join(z_pickup,\n",
    "                        trip_df['\"PULocationID\"']==z_pickup[\"Pu_location_id\"]\n",
    "                        ).join(z_drop,\n",
    "                               trip_df['\"DOLocationID\"']==z_drop[\"do_location_id\"]\n",
    "                               ).join(weather_df,\n",
    "                                      trip_df['\"RideDate\"'] == weather_df['\"ODate\"'])\n",
    "\n",
    "    final_df= join_df.with_column('\"PickupAddress\"',\n",
    "                                  concat_ws(lit(','),join_df[\"pu_zone\"],join_df[\"pu_borough\"])\n",
    "                                  ).with_column('\"DropAddress\"',\n",
    "                                                concat_ws(lit(','),join_df[\"do_zone\"],join_df[\"do_borough\"]))\n",
    "\n",
    "    # final_df = trip_df.join(\n",
    "    #     weather_df,\n",
    "    #     trip_df['\"RideDate\"'] == weather_df['\"ODate\"'],\n",
    "    #     join_type = \"inner\"\n",
    "    # )\n",
    "\n",
    "    final_df=final_df.select('\"VendorID\"','\"PickupAddress\"','\"DropAddress\"','\"PickupTime\"','\"DropoffTime\"','\"TripDistance\"','\"TotalAmount\"','\"Tmin\"','\"Tmax\"','\"Prcp\"','\"Snow\"','\"Snwd\"','\"Awnd\"','\"Wsf2\"','\"Wdf2\"','\"Wsf5\"','\"Wdf5\"')\n",
    "    final_df.write.mode(\"overwrite\").saveAsTable(final_table)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
