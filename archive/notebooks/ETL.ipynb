{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Replace with your local file path\n",
    "# parquet_file = \"/Users/charlessanthakumar/Documents/POC/yellow_tripdata_2025-05.parquet\"\n",
    "\n",
    "# # Read Parquet file\n",
    "# df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# # View the first 5 rows\n",
    "# print(df.head())\n",
    "\n",
    "# # View columns\n",
    "# print(df.columns)\n",
    "\n",
    "# # View basic info\n",
    "# #print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = \"taxi_zone_lookup.csv\"\n",
    "\n",
    "# df_csv = pd.read_csv(csv_file)\n",
    "# print(\"\\nFirst 5 rows of the CSV file:\")\n",
    "# print(df_csv.head())\n",
    "# print(\"\\nCSV Columns:\")\n",
    "# print(df_csv.columns)\n",
    "# print(\"\\nCSV Info:\")\n",
    "# # print(df_csv.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37486c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   date datatype            station attributes  value\n",
      "0   2025-05-01T00:00:00     AWND  GHCND:USW00094728       ,,W,    2.4\n",
      "1   2025-05-01T00:00:00     PRCP  GHCND:USW00094728   ,,W,2400    0.0\n",
      "2   2025-05-01T00:00:00     SNOW  GHCND:USW00094728       ,,W,    0.0\n",
      "3   2025-05-01T00:00:00     SNWD  GHCND:USW00094728   ,,W,2400    0.0\n",
      "4   2025-05-01T00:00:00     TMAX  GHCND:USW00094728   ,,W,2400   21.7\n",
      "..                  ...      ...                ...        ...    ...\n",
      "95  2025-05-09T00:00:00     WDF5  GHCND:USW00094728       ,,W,   60.0\n",
      "96  2025-05-09T00:00:00     WSF2  GHCND:USW00094728       ,,W,    7.2\n",
      "97  2025-05-09T00:00:00     WSF5  GHCND:USW00094728       ,,W,   10.7\n",
      "98  2025-05-09T00:00:00     WT01  GHCND:USW00094728       ,,W,    1.0\n",
      "99  2025-05-10T00:00:00     AWND  GHCND:USW00094728       ,,W,    2.5\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "Index(['date', 'datatype', 'station', 'attributes', 'value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "#import pandas as pd\n",
    "\n",
    "# Replace with your NOAA API token\n",
    "TOKEN = \"uuhabihGBDGcWfSkBUcWIraYGqSscifq\"\n",
    "\n",
    "# Define endpoint and parameters\n",
    "endpoint = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "\n",
    "headers = {\"token\": TOKEN}\n",
    "\n",
    "# Example: Fetch daily summaries for Central Park Station (USW00094728)\n",
    "params = {\n",
    "    \"datasetid\": \"GHCND\",\n",
    "    \"stationid\": \"GHCND:USW00094728\",\n",
    "    \"startdate\": \"2025-05-01\",\n",
    "    \"enddate\": \"2025-05-31\",\n",
    "    \"units\": \"metric\",\n",
    "    \"limit\": 1000\n",
    "}\n",
    "\n",
    "response = requests.get(endpoint, headers=headers, params=params)\n",
    "output_path = \"weather_data_pivoted.csv\"  # or \"nyc_taxi_weather_enriched.csv\"\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    df = pd.json_normalize(data['results'])\n",
    "    print(df.head(100))\n",
    "    print(df.columns)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import pandas as pd\n",
    "\n",
    "# # Read your CSV\n",
    "# df = pd.read_csv(\"weather_data_pivoted.csv\")\n",
    "\n",
    "# # Display the original DataFrame\n",
    "# print(\"Original Data:\")\n",
    "# print(df)\n",
    "\n",
    "# # Keep only relevant columns\n",
    "# df_clean = df[['date', 'datatype', 'value']]\n",
    "\n",
    "# # Pivot the datatype into columns, values filled from 'value'\n",
    "# df_pivot = df_clean.pivot_table(\n",
    "#     index='date',\n",
    "#     columns='datatype',\n",
    "#     values='value'\n",
    "# ).reset_index()\n",
    "\n",
    "# # Flatten column MultiIndex if needed\n",
    "# df_pivot.columns.name = None\n",
    "\n",
    "# # Display the pivoted DataFrame\n",
    "# print(\"\\nPivoted Data:\")\n",
    "# print(df_pivot)\n",
    "\n",
    "# # Save to CSV if desired\n",
    "# df_pivot.to_csv(\"weather_pivoted.csv\", index=False)\n",
    "# print(\"\\nPivoted data saved to weather_pivoted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83494659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import pandas as pd\n",
    "# class NYCTripDataProcessor:\n",
    "#     def __init__(self, file_path):\n",
    "#         self.file_path = file_path\n",
    "#         self.df = pd.read_parquet(file_path)\n",
    "\n",
    "#     def get_trip_data(self):\n",
    "#         return self.df\n",
    "\n",
    "#     def get_columns(self):\n",
    "#         return self.df.columns.tolist()\n",
    "\n",
    "#     def get_info(self):\n",
    "#         return self.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d786d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataProcessor:\n",
    "    def __init__(self, token, endpoint):\n",
    "        self.token = token\n",
    "        self.endpoint = endpoint\n",
    "        self.headers = {\"token\": self.token}\n",
    "        \n",
    "    def fetch_weather_data(self, dataset_id, station_id, start_date, end_date, units):\n",
    "        params = {\n",
    "            \"datasetid\": dataset_id,\n",
    "            \"stationid\": station_id,\n",
    "            \"startdate\": start_date,\n",
    "            \"enddate\": end_date,\n",
    "            \"units\": units,\n",
    "            #\"limit\": 1000\n",
    "        }\n",
    "        response = requests.get(self.endpoint, headers=self.headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return pd.json_normalize(data['results'])\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "        \n",
    "    def pivot_weather_data(self, df):\n",
    "        df_clean = df[['date', 'datatype', 'value']]\n",
    "        df_pivot = df_clean.pivot_table(\n",
    "            index='date',\n",
    "            columns='datatype',\n",
    "            values='value'\n",
    "        ).reset_index()\n",
    "        df_pivot.columns.name = None\n",
    "        return df_pivot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9874e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #not to be considered\n",
    "# class DQChecker:\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    "\n",
    "#     def check_nulls(self):\n",
    "#         return self.df.isnull().sum()\n",
    "\n",
    "#     def check_duplicates(self):\n",
    "#         return self.df.duplicated().sum()\n",
    "\n",
    "#     def check_data_types(self):\n",
    "#         return self.df.dtypes\n",
    "\n",
    "#     def run_all_checks(self):\n",
    "#         return {\n",
    "#             \"nulls\": self.check_nulls(),\n",
    "#             \"duplicates\": self.check_duplicates(),\n",
    "#             \"data_types\": self.check_data_types()\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e134fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class utils:\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_to_csv(df, file_name):\n",
    "        df.to_csv(file_name, index=False)\n",
    "        print(f\"Data saved to {file_name}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_csv(file_name):\n",
    "        return pd.read_csv(file_name)\n",
    "    @staticmethod\n",
    "    def derive_date_columns(df, date_column, new_column_name):\n",
    "        # derive date from timestamp and append to the DataFrame\n",
    "        df[new_column_name] = pd.to_datetime(df[date_column]).dt.date\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching trip data...\n",
      "Trip Data Columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee']\n",
      "Trip Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4591845 entries, 0 to 4591844\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int32         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int32         \n",
      " 8   DOLocationID           int32         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  Airport_fee            float64       \n",
      " 19  cbd_congestion_fee     float64       \n",
      "dtypes: datetime64[us](2), float64(13), int32(3), int64(1), object(1)\n",
      "memory usage: 648.1+ MB\n",
      "\n",
      "Fetching weather data from API...\n",
      "Sample data from Weather Dataset\n",
      "                   date datatype            station attributes  value\n",
      "0  2025-05-01T00:00:00     AWND  GHCND:USW00094728       ,,W,    2.4\n",
      "1  2025-05-01T00:00:00     PRCP  GHCND:USW00094728   ,,W,2400    0.0\n",
      "2  2025-05-01T00:00:00     SNOW  GHCND:USW00094728       ,,W,    0.0\n",
      "3  2025-05-01T00:00:00     SNWD  GHCND:USW00094728   ,,W,2400    0.0\n",
      "4  2025-05-01T00:00:00     TMAX  GHCND:USW00094728   ,,W,2400   21.7\n",
      "Weather Data Columns: Index(['date', 'datatype', 'station', 'attributes', 'value'], dtype='object')\n",
      "Pivoting weather data...\n",
      "Pivoted Weather Data:\n",
      "                  date  AWND  PRCP  SNOW  SNWD  TMAX  TMIN   WDF2   WDF5  \\\n",
      "0  2025-05-01T00:00:00   2.4   0.0   0.0   0.0  21.7  11.1  130.0  130.0   \n",
      "1  2025-05-02T00:00:00   1.0   0.0   0.0   0.0  28.9  13.9  230.0  240.0   \n",
      "2  2025-05-03T00:00:00   1.3  24.1   0.0   NaN   NaN   NaN    NaN    NaN   \n",
      "\n",
      "   WSF2  WSF5  WT01  WT08  \n",
      "0   5.8   9.4   NaN   NaN  \n",
      "1   6.3  10.3   1.0   1.0  \n",
      "2   NaN   NaN   NaN   NaN  \n",
      "Deriving date column for weather data...\n",
      "Deriving date column from trip data...\n",
      "Sample data after deriving date column:\n",
      "                  date  AWND  PRCP  SNOW  SNWD  TMAX  TMIN   WDF2   WDF5  \\\n",
      "0  2025-05-01T00:00:00   2.4   0.0   0.0   0.0  21.7  11.1  130.0  130.0   \n",
      "1  2025-05-02T00:00:00   1.0   0.0   0.0   0.0  28.9  13.9  230.0  240.0   \n",
      "2  2025-05-03T00:00:00   1.3  24.1   0.0   NaN   NaN   NaN    NaN    NaN   \n",
      "\n",
      "   WSF2  WSF5  WT01  WT08 weather_date  \n",
      "0   5.8   9.4   NaN   NaN   2025-05-01  \n",
      "1   6.3  10.3   1.0   1.0   2025-05-02  \n",
      "2   NaN   NaN   NaN   NaN   2025-05-03  \n",
      "Sample data after deriving date column from trip data:\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2025-05-01 00:07:06   2025-05-01 00:24:15              1.0   \n",
      "1         2  2025-05-01 00:07:44   2025-05-01 00:14:27              1.0   \n",
      "2         2  2025-05-01 00:15:56   2025-05-01 00:23:53              1.0   \n",
      "3         2  2025-05-01 00:00:09   2025-05-01 00:25:29              1.0   \n",
      "4         2  2025-05-01 00:45:07   2025-05-01 00:52:45              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           3.70         1.0                  N           140           202   \n",
      "1           1.03         1.0                  N           234           161   \n",
      "2           1.57         1.0                  N           161           234   \n",
      "3           9.48         1.0                  N           138            90   \n",
      "4           1.80         1.0                  N            90           231   \n",
      "\n",
      "   payment_type  ...  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             1  ...      0.5        4.85          0.00   \n",
      "1             1  ...      0.5        4.30          0.00   \n",
      "2             2  ...      0.5        0.00          0.00   \n",
      "3             1  ...      0.5       11.70          6.94   \n",
      "4             1  ...      0.5        1.50          0.00   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \\\n",
      "0                    1.0         29.00                   2.5         0.00   \n",
      "1                    1.0         18.65                   2.5         0.00   \n",
      "2                    1.0         15.75                   2.5         0.00   \n",
      "3                    1.0         71.94                   2.5         1.75   \n",
      "4                    1.0         17.25                   2.5         0.00   \n",
      "\n",
      "   cbd_congestion_fee  pickup_date  dropoff_date  \n",
      "0                0.75   2025-05-01    2025-05-01  \n",
      "1                0.75   2025-05-01    2025-05-01  \n",
      "2                0.75   2025-05-01    2025-05-01  \n",
      "3                0.75   2025-05-01    2025-05-01  \n",
      "4                0.75   2025-05-01    2025-05-01  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trip_data_file_path = \"yellow_tripdata_2025-05.parquet\"\n",
    "    zone_lookup_file_path = \"taxi_zone_lookup.csv\"\n",
    "\n",
    "    weather_token = \"uuhabihGBDGcWfSkBUcWIraYGqSscifq\"\n",
    "    weather_endpoint = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "    dataset_id = \"GHCND\"\n",
    "    station_id = \"GHCND:USW00094728\"\n",
    "    start_date = \"2025-05-01\"\n",
    "    end_date = \"2025-05-31\"\n",
    "    units = \"metric\"\n",
    "\n",
    "    \n",
    "\n",
    "    # Initialize Nyc trip data processor\n",
    "    trip_data_processor = NYCTripDataProcessor(trip_data_file_path)\n",
    "    #zone_mapping = NYCTripDataProcessor(zone_lookup_file_path)\n",
    "    weather_data_processor = WeatherDataProcessor(weather_token, weather_endpoint)\n",
    "    #utils_instance = utils()\n",
    "\n",
    "    print(\"Fetching trip data...\")\n",
    "    trip_df = trip_data_processor.get_trip_data()\n",
    "    #zone_mapping_df = zone_mapping.get_trip_data()\n",
    "\n",
    "    print(\"Trip Data Columns:\", trip_data_processor.get_columns())\n",
    "    #print(\"Zone Mapping Columns:\", zone_mapping_df.get_columns())\n",
    "\n",
    "    print(\"Trip Data Info:\")\n",
    "    trip_data_processor.get_info()\n",
    "\n",
    "    print(\"\\nFetching weather data from API...\")\n",
    "    weather_df = weather_data_processor.fetch_weather_data(dataset_id, station_id, start_date, end_date, units)\n",
    "\n",
    "    print(\"Sample data from Weather Dataset\\n\", weather_df.head())\n",
    "    print(\"Weather Data Columns:\", weather_df.columns)\n",
    "\n",
    "    print(\"Pivoting weather data...\")\n",
    "    weather_pivoted_df = weather_data_processor.pivot_weather_data(weather_df)\n",
    "\n",
    "    print(\"Pivoted Weather Data:\")\n",
    "    print(weather_pivoted_df.head())\n",
    "\n",
    "    print(\"Deriving date column for weather data...\")\n",
    "    #weather_pivoted_df = utils_instance.derive_date_columns(weather_pivoted_df, 'date', 'weather_date')\n",
    "\n",
    "    weather_pivoted_df = utils.derive_date_columns(weather_pivoted_df, 'date', 'weather_date')\n",
    "\n",
    "    print(\"Deriving date column from trip data...\")\n",
    "    trip_df = utils.derive_date_columns(trip_df, 'tpep_pickup_datetime', 'pickup_date')\n",
    "    trip_df = utils.derive_date_columns(trip_df, 'tpep_dropoff_datetime', 'dropoff_date')\n",
    "\n",
    "    print(\"Sample data after deriving date column:\")\n",
    "    print(weather_pivoted_df.head())\n",
    "\n",
    "    print(\"Sample data after deriving date column from trip data:\")\n",
    "    print(trip_df.head())\n",
    "\n",
    "    # dqc = DQChecker(trip_df)\n",
    "    # print(\"\\nRunning Data Quality Checks on Trip Data:\")\n",
    "    # dqc_results = dqc.run_all_checks()\n",
    "    # print(\"Null Values:\\n\", dqc_results['nulls'])\n",
    "    # print(\"Duplicate Rows:\", dqc_results['duplicates'])\n",
    "    # print(\"Data Types:\\n\", dqc_results['data_types'])   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a741034",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "duplicate_check(\n",
    "    df_trip_orig,\n",
    "    session,\n",
    "    \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_DUPLICATES\"\n",
    ")\n",
    "\n",
    "df_trip_orig = session.table(\"INBOUND_INTEGRATION.LANDING_TRIP.YELLOW_TRIP_RECORDS\")\n",
    "df_weather = session.table(\"INBOUND_INTEGRATION.LANDING_WEATHER.NYC_WEATHER\")\n",
    "#print(\"weather data\")\n",
    "#df_weather.show()\n",
    "\n",
    "# Call null_check as usual\n",
    "null_count = null_check(df_trip_orig, session, \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_NULL_RECORDS\")\n",
    "#null_count = null_check(df_weather, session, \"INBOUND_INTEGRATION.DQ_WEATHER.WEATHER_NULL_RECORDS\")\n",
    "\n",
    "if null_count == 0:\n",
    "    print(\"Proceeding to next DQ checks.\")\n",
    "else:\n",
    "    print(f\"{null_count} rows with null/empty values found. Review DQ_TRIPDATA_NULLS table.\")\n",
    "\n",
    "trip_int_columns = [ '\"VendorID\"',\n",
    "    '\"PULocationID\"', '\"DOLocationID\"'\n",
    "]\n",
    "\n",
    "trip_ts_columns = [\n",
    "    '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"'\n",
    "]\n",
    "\n",
    "#df_junk = junk_value_check(df_trip_orig, trip_int_columns)\n",
    "df_junk_t = junk_value_check(df_trip_orig, trip_ts_columns, type=\"timestamp_type\")\n",
    "df_junk_i = junk_value_check(df_trip_orig, trip_int_columns, type=\"integer_type\")\n",
    "df_junk_t.show()\n",
    "df_junk_i.show()\n",
    "print(f\"Junk records found: {df_junk_t.count() + df_junk_t.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69291530",
   "metadata": {},
   "source": [
    "## Above logic to be discarded - pandas based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d38eb42",
   "metadata": {},
   "source": [
    "# Landing population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a3067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snowflake-snowpark-python\n",
      "  Downloading snowflake_snowpark_python-1.35.0-py3-none-any.whl.metadata (150 kB)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-snowpark-python) (65.5.0)\n",
      "Collecting wheel (from snowflake-snowpark-python)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting snowflake-connector-python<4.0.0,>=3.14.0 (from snowflake-snowpark-python)\n",
      "  Downloading snowflake_connector_python-3.16.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-snowpark-python) (4.13.2)\n",
      "Collecting pyyaml (from snowflake-snowpark-python)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting cloudpickle!=2.1.0,!=2.2.0,<=3.0.0,>=1.6.0 (from snowflake-snowpark-python)\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf<6,>=3.20 (from snowflake-snowpark-python)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: python-dateutil in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-snowpark-python) (2.9.0.post0)\n",
      "Collecting tzlocal (from snowflake-snowpark-python)\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting boto3>=1.24 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Downloading boto3-1.40.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore>=1.24 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Downloading botocore-1.40.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting cffi<2.0.0,>=1.9 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Downloading cffi-1.17.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting cryptography>=3.1.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached cryptography-45.0.5-cp37-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting pyOpenSSL<26.0.0,>=22.0.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached pyopenssl-25.1.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pyjwt<3.0.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pytz in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2025.2)\n",
      "Requirement already satisfied: requests<3.0.0 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.32.3)\n",
      "Requirement already satisfied: packaging in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2025.1.31)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (3.18.0)\n",
      "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (4.3.7)\n",
      "Collecting tomlkit (from snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from python-dateutil->snowflake-snowpark-python) (1.17.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/charlessanthakumar/.pyenv/versions/3.10.13/envs/cleanenv310/lib/python3.10/site-packages (from botocore>=1.24->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python) (2.4.0)\n",
      "Collecting pycparser (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=3.14.0->snowflake-snowpark-python)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading snowflake_snowpark_python-1.35.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Downloading snowflake_connector_python-3.16.0-cp310-cp310-macosx_11_0_arm64.whl (993 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.1/993.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Downloading boto3-1.40.2-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.2-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.17.1-cp310-cp310-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached cryptography-45.0.5-cp37-abi3-macosx_10_9_universal2.whl (7.0 MB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached pyopenssl-25.1.0-py3-none-any.whl (56 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, asn1crypto, wheel, tzlocal, tomlkit, pyyaml, pyjwt, pycparser, protobuf, jmespath, cloudpickle, cffi, botocore, s3transfer, cryptography, pyOpenSSL, boto3, snowflake-connector-python, snowflake-snowpark-python\n",
      "Successfully installed asn1crypto-1.5.1 boto3-1.40.2 botocore-1.40.2 cffi-1.17.1 cloudpickle-3.0.0 cryptography-45.0.5 jmespath-1.0.1 protobuf-5.29.5 pyOpenSSL-25.1.0 pycparser-2.22 pyjwt-2.10.1 pyyaml-6.0.2 s3transfer-0.13.1 snowflake-connector-python-3.16.0 snowflake-snowpark-python-1.35.0 sortedcontainers-2.4.0 tomlkit-0.13.3 tzlocal-5.3.1 wheel-0.45.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install snowflake-snowpark-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2671a263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\",override=True)\n",
    "print(os.getcwd())\n",
    "\n",
    "print(os.environ[\"SNOWFLAKE_ACCOUNT\"])\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # try without path first\n",
    "\n",
    "# for k in [\"SNOWFLAKE_ACCOUNT\", \"SNOWFLAKE_USER\", \"SNOWFLAKE_PASSWORD\", \"SNOWFLAKE_ROLE\"]:\n",
    "#     print(f\"{k} = {os.getenv(k)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc433940",
   "metadata": {},
   "source": [
    "# Weather data schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark.types import StructType, StructField, StringType\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"date\", StringType()),\n",
    "#     StructField(\"datatype\", StringType()),\n",
    "#     StructField(\"station\", StringType()),\n",
    "#     StructField(\"attributes\", StringType()),\n",
    "#     StructField(\"value\", StringType())  # stored as string even if numeric\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5ca7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather API data ingested directly into Snowflake landing table using Snowpark without pandas.\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "# Snowflake connection\n",
    "connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "   # \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"INBOUND_INTEGRATION\",\n",
    "    \"schema\": \"LANDING_WEATHER\"\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "# Fetch weather data from API\n",
    "TOKEN = os.environ[\"NCEI_TOKEN\"]\n",
    "endpoint = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "headers = {\"token\": TOKEN}\n",
    "params = {\n",
    "    \"datasetid\": \"GHCND\",\n",
    "    \"stationid\": \"GHCND:USW00094728\",\n",
    "    \"startdate\": \"2025-05-01\",\n",
    "    \"enddate\": \"2025-05-31\",\n",
    "    \"units\": \"metric\",\n",
    "    \"limit\": 1000\n",
    "}\n",
    "\n",
    "response = requests.get(endpoint, headers=headers, params=params)\n",
    "data = response.json()[\"results\"]  # This is already a list of dicts\n",
    "\n",
    "\n",
    "\n",
    "#for row in data[:5]:\n",
    "#    print(row)\n",
    "\n",
    "# Create Snowpark DataFrame directly\n",
    "df_snowpark = session.create_dataframe(data,schema=schema)\n",
    "\n",
    "# Write to Snowflake landing table\n",
    "df_snowpark.write.mode(\"overwrite\").save_as_table(\"INBOUND_INTEGRATION.LANDING_WEATHER.NYC_WEATHER\")\n",
    "\n",
    "print(\"Weather API data ingested directly into Snowflake landing table using Snowpark without pandas.\")\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450513e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file ingested into YELLOW_TRIP_RECORDS successfully.\n",
      "---------------------------------------------------------------------------\n",
      "|\"LOCATIONID\"  |\"BOROUGH\"      |\"ZONE\"                   |\"SERVICE_ZONE\"  |\n",
      "---------------------------------------------------------------------------\n",
      "|1             |EWR            |Newark Airport           |EWR             |\n",
      "|2             |Queens         |Jamaica Bay              |Boro Zone       |\n",
      "|3             |Bronx          |Allerton/Pelham Gardens  |Boro Zone       |\n",
      "|4             |Manhattan      |Alphabet City            |Yellow Zone     |\n",
      "|5             |Staten Island  |Arden Heights            |Boro Zone       |\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.functions import col \n",
    "\n",
    "# Session connection\n",
    "connection_parameters = {\n",
    "    \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "   # \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"INBOUND_INTEGRATION\",\n",
    "    \"schema\": \"LANDING_TRIP\"\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "#  Read Parquet file from stage\n",
    "#df_trip = session.read.parquet(\"@LANDING_TRIP_STAGE/yellow_tripdata_2025-05.parquet\")\n",
    "\n",
    "#df_trip.show(5)\n",
    "\n",
    "#  Write to your landing table (overwrite or append)\n",
    "#df_trip.write.mode(\"append\").save_as_table(\"YELLOW_TRIP_RECORDS\")\n",
    "\n",
    "print(\"Parquet file ingested into YELLOW_TRIP_RECORDS successfully.\")\n",
    "\n",
    "df_lookup = session.read.options({\n",
    "    \"FIELD_OPTIONALLY_ENCLOSED_BY\": '\"',\n",
    "    \"SKIP_HEADER\": 0\n",
    "}).csv(\"@landing_trip_stage/taxi_zone_lookup.csv.gz\")\n",
    "\n",
    "lookup_headers = df_lookup.first()\n",
    "#lookup_schema = StructType([StructField(cols.strip(), StringType()) for cols in lookup_headers])\n",
    "\n",
    "df_lookup = df_lookup.to_df(*lookup_headers) #creates new dataframe with columns as provided list\n",
    "\n",
    "\n",
    "# Using filter to remove the header row based on the first column value\n",
    "df_lookup = df_lookup.filter(col(lookup_headers[0]) != lookup_headers[0])\n",
    "\n",
    "df_lookup.show(5)\n",
    "df_lookup.write.mode(\"overwrite\").save_as_table(\"TAXI_ZONE_LOOKUP\")\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd9f4c8",
   "metadata": {},
   "source": [
    "# DQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key_columns_needed = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID']\n",
    "t_data_key_columns = [f'\"{col}\"' for col in key_columns_needed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a37d573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_columns_needed = ['DATE', 'DATATYPE']\n",
    "t_data_key_columns = [f'\"{col}\"' for col in key_columns_needed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ba6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark.functions import col, lit, trim\n",
    "\n",
    "# def build_null_condition(key_columns):\n",
    "#     \"\"\"\n",
    "#     Builds a condition to check for null or empty values in key columns.\n",
    "#     \"\"\"\n",
    "#     null_condition = None\n",
    "#     for column in key_columns:\n",
    "#         #clean_col = column.strip('\"')\n",
    "#         condition = col(column).is_null() |  (trim(col(column)) == lit(\"\"))\n",
    "#         null_condition = condition if null_condition is None else null_condition | condition\n",
    "#     return null_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from snowflake.snowpark import Session\n",
    "# from snowflake.snowpark.functions import col, lit\n",
    "# # Session connection\n",
    "# connection_parameters = {\n",
    "#     \"account\": os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "#     \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "#     \"password\": os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "#     \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "#    # \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "#     \"database\": \"INBOUND_INTEGRATION\",\n",
    "#     \"schema\": \"LANDING_TRIP\"\n",
    "# }\n",
    "\n",
    "# session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "# def null_check(df, session, dq_table_name):\n",
    "#     \"\"\"\n",
    "#     Checks for null/empty values in critical composite key columns\n",
    "#     and stores violating rows in a DQ table.\n",
    "#     \"\"\"\n",
    "#     cond = build_null_condition(t_data_key_columns)\n",
    "\n",
    "#     print(cond)\n",
    "\n",
    "#     print (\"Invalid rows with null/empty values in key columns\")\n",
    "#     df_invalid = df.filter(cond)\n",
    "#     #print(df_invalid.schema.names)\n",
    "#     df_invalid.show()\n",
    "#     df_valid = df.filter(~cond)\n",
    "#     print(\"Valid rows with no null/empty values in key columns\")\n",
    "#     df_valid.show()\n",
    "\n",
    "#     print(\"Total:\", df.count())\n",
    "#     print(\"Invalid (nulls):\", df.filter(cond).count())\n",
    "#     print(\"Valid:\", df.filter(~cond).count())\n",
    "#     #print(df_invalid.count())\n",
    "#     if df_invalid.count() > 0:\n",
    "#         print(f\"Found {df_invalid.count()} rows with null/empty values in key columns.\")\n",
    "#         df_invalid.write.mode(\"overwrite\").save_as_table(dq_table_name)\n",
    "\n",
    "#     else:\n",
    "#         print(\"No null/empty values found in key columns.\")\n",
    "#     return df_invalid.count()\n",
    "\n",
    "# df_trip_orig = session.table(\"INBOUND_INTEGRATION.LANDING_TRIP.YELLOW_TRIP_RECORDS\")\n",
    "# df_weather = session.table(\"INBOUND_INTEGRATION.LANDING_WEATHER.NYC_WEATHER\")\n",
    "# print(\"weather data\")\n",
    "# #df_weather.show()\n",
    "# # Strip quotes immediately:\n",
    "# #cleaned_columns = [col_name.strip('\"') for col_name in df_trip_orig.schema.names]\n",
    "# #print(cleaned_columns)\n",
    "\n",
    "\n",
    "# # Call null_check as usual\n",
    "# null_count = null_check(df_trip_orig, session, \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_NULL_RECORDS\")\n",
    "# #null_count = null_check(df_weather, session, \"INBOUND_INTEGRATION.DQ_WEATHER.WEATHER_NULL_RECORDS\")\n",
    "\n",
    "\n",
    "# if null_count == 0:\n",
    "#     print(\"✅ Proceeding to next DQ checks.\")\n",
    "# else:\n",
    "#     print(f\"⚠️ {null_count} rows with null/empty values found. Review DQ_TRIPDATA_NULLS table.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark.functions import col, count, lit, row_number\n",
    "# from snowflake.snowpark.window import Window\n",
    "\n",
    "\n",
    "# def duplicate_check(df, session, dq_table_name):\n",
    "#     \"\"\"\n",
    "#     Checks for duplicate rows based on composite key columns\n",
    "#     and stores violating rows in a DQ table.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # introduce row number to append only the first occurence of the duplicated records to clean and the rest to duplicates tables\n",
    "#     df_full_dedup = df.with_column('rn', \n",
    "#         (row_number().over(Window.partition_by([col(c) for c in t_data_key_columns]).order_by([col(c) for c in t_data_key_columns]))))\n",
    "#     df_full_dedup.show()\n",
    "#     df_full_dedup_clean = df_full_dedup.filter(col('rn') == 1)\n",
    "#     df_full_dedup = df_full_dedup.filter(col('rn') != 1).drop('rn')\n",
    "#     # To be passed to junk check \n",
    "#     #df_clean_final = df_clean.union(df_full_dedup_clean)\n",
    "\n",
    "#     clean_count = df_full_dedup_clean.count()\n",
    "#     print (f\"{clean_count} clean records\")\n",
    "\n",
    "#     dupe_count = df_full_dedup.count()\n",
    "    \n",
    "#     #df_duplicates.show()\n",
    "\n",
    "#     if dupe_count > 0:\n",
    "#         print(f\"{dupe_count} duplicate rows found. Recording to {dq_table_name}\")\n",
    "#         df_full_dedup.write.mode(\"overwrite\").save_as_table(dq_table_name)\n",
    "#     else:\n",
    "#         print(\"Duplicate check passed: No duplicate found\")\n",
    "\n",
    "#     return dupe_count\n",
    "\n",
    "# duplicate_check(\n",
    "#     df_trip_orig,\n",
    "#     session,\n",
    "#     \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_DUPLICATES\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ignore this block\n",
    "\n",
    "# from snowflake.snowpark.functions import col, count, lit\n",
    "\n",
    "# key_columns_needed = ['\"VendorID\"', '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"', '\"PULocationID\"', '\"DOLocationID\"']\n",
    "# #key_columns_needed = ['\"date\"', '\"datatype\"']\n",
    "# t_data_key_columns = key_columns_needed\n",
    "\n",
    "\n",
    "# df_duplicates = (\n",
    "#     df_trip_orig.group_by([col(c) for c in t_data_key_columns])\n",
    "#     .agg(count(lit(1)).alias(\"record_count\"))\n",
    "#     .filter(col(\"record_count\") > 1)\n",
    "# )\n",
    "\n",
    "# print(df_duplicates.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7095151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark.functions import col, trim, try_cast\n",
    "# from snowflake.snowpark.types import IntegerType,TimestampType\n",
    "\n",
    "# trip_int_columns = [ '\"VendorID\"',\n",
    "#     '\"PULocationID\"', '\"DOLocationID\"'\n",
    "# ]\n",
    "\n",
    "# trip_ts_columns = [\n",
    "#     '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"'\n",
    "# ]\n",
    "\n",
    "\n",
    "# def junk_value_check(df, numeric_columns):\n",
    "#     \"\"\"\n",
    "#     Detects non-numeric (junk) values in string columns that are expected to be numeric,\n",
    "#     using regex and ignoring null/empty strings.\n",
    "#     \"\"\"\n",
    "#     junk_condition = None\n",
    "#     for column in numeric_columns:\n",
    "#         trimmed_col = trim(col(column))\n",
    "#         # Matches anything NOT a valid number: not digits, not optional dot, not optional minus\n",
    "#         #condition = try_cast(trimmed_col, IntegerType() ).is_null()\n",
    "#         condition = try_cast(trimmed_col, TimestampType() ).is_null()\n",
    "#         junk_condition = condition if junk_condition is None else (junk_condition | condition)\n",
    "    \n",
    "#     return df.filter(junk_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark.types import IntegerType, TimestampType\n",
    "\n",
    "# # Define expected types per column - later to belong to config\n",
    "# expected_types = {\n",
    "#     '\"PULocationID\"': IntegerType(),\n",
    "#     '\"DOLocationID\"': IntegerType(),\n",
    "#     '\"tpep_pickup_datetime\"': TimestampType(),\n",
    "#     '\"tpep_dropoff_datetime\"': TimestampType()\n",
    "# }\n",
    "\n",
    "# #df_junk = junk_value_check(df_trip_orig, trip_int_columns)\n",
    "# df_junk = junk_value_check(df_trip_orig, trip_ts_columns)\n",
    "# df_junk.show()\n",
    "# print(f\"Junk records found: {df_junk.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30d604",
   "metadata": {},
   "source": [
    "### Working DQ timestamp check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b18f906",
   "metadata": {},
   "outputs": [],
   "source": [

    "# from snowflake.snowpark.functions import col, trim, try_cast\n",
    "\n",
    "# def junk_value_check(df, columns, type, min_valid_epoch=1600000000000):\n",
    "#     \"\"\"\n",
    "#     Flags rows where timestamp fields are present but invalid:\n",
    "#     - Either not castable to integer\n",
    "#     - Or below a minimum valid epoch threshold\n",
    "#     \"\"\"\n",
    "#     junk_condition = None\n",
    "\n",
    "#     for column in columns:\n",
    "#         trimmed_col = trim(col(column))\n",
    "#         casted_col = try_cast(trimmed_col, IntegerType())\n",
    "#         if type == \"timestamp_type\":\n",
    "#             condition = ((casted_col.is_null()) | (casted_col < min_valid_epoch))\n",
    "#         else :\n",
    "#               condition = (casted_col.is_null())\n",
    "\n",
    "#         junk_condition = condition if junk_condition is None else (junk_condition | condition)\n",
    "\n",
    "#     return df.filter(junk_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snowflake.snowpark.types import IntegerType, TimestampType\n",
    "\n",
    "# # Define expected types per column - later to belong to config\n",
    "# expected_types = {\n",
    "#     '\"PULocationID\"': IntegerType(),\n",
    "#     '\"DOLocationID\"': IntegerType(),\n",
    "#     '\"tpep_pickup_datetime\"': TimestampType(),\n",
    "#     '\"tpep_dropoff_datetime\"': TimestampType()\n",
    "# }\n",
    "\n",
    "# trip_int_columns = [ '\"VendorID\"',\n",
    "#     '\"PULocationID\"', '\"DOLocationID\"'\n",
    "# ]\n",
    "\n",
    "# trip_ts_columns = [\n",
    "#     '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"'\n",
    "# ]\n",
    "\n",
    "# df_junk_t = junk_value_check(df_trip_orig, trip_ts_columns, type=\"timestamp_type\")\n",
    "# df_junk_i = junk_value_check(df_trip_orig, trip_int_columns, type=\"integer_type\")\n",
    "# df_junk_t.show()\n",
    "# df_junk_i.show()\n",
    "# print(f\"Junk records found: {df_junk_t.count() + df_junk_t.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0d6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 8\n"
     ]
    },
    {

     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",

    "load_dotenv(\"/Users/charlessanthakumar/Documents/Trip-Analytics/set_sf_env.sh\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27cd4d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account in connection_parameters: 'OB52901'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Snowflake connection\n",
    "connection_parameters = {\n",
    "    \"account\": os.environ[\"TF_VAR_snowflake_account_name\"],\n",
    "    #\"organization\": os.environ[\"TF_VAR_snowflake_organization_name\"],\n",
    "    \"user\": os.environ[\"TF_VAR_snowflake_user\"],\n",
    "    \"password\": os.environ[\"TF_VAR_snowflake_password\"],\n",
    "    \"role\": os.environ[\"TF_VAR_snowflake_role\"],\n",
    "   # \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "    \"database\": \"INBOUND_INTEGRATION\",\n",
    "    \"schema\": \"LANDING_WEATHER\"\n",
    "}\n",
    "print(\"Account in connection_parameters:\", repr(connection_parameters[\"account\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,

   "id": "ba87f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col, lit, trim, row_number, try_cast\n",
    "from snowflake.snowpark.window import Window\n",
    "from snowflake.snowpark.types import IntegerType,TimestampType\n",
    "\n",
    "class DQCheck :\n",
    "\n",
    "    def __init__(self,session,key_columns, dq_table_name ):\n",
    "        self.session = session\n",
    "        self.key_columns = key_columns\n",
    "        #self.t_key_columns = t_key_columns\n",
    "        #self.w_key_columns = w_key_columns\n",
    "        self.dq_table_name = dq_table_name\n",
    "\n",
    "    def build_null_condition(self):\n",
    "        \"\"\"\n",
    "        Builds a condition to check for null or empty values in key columns.\n",
    "        \"\"\"\n",
    "        null_condition = None\n",
    "        for column in self.key_columns:\n",
    "            #clean_col = column.strip('\"')\n",
    "            condition = col(column).is_null() |  (trim(col(column)) == lit(\"\"))\n",
    "            null_condition = condition if null_condition is None else null_condition | condition\n",
    "        return null_condition\n",
    "    \n",
    "\n",

    "    def null_check(self,df,dq_table_name):\n",

    "        \"\"\"\n",
    "        Checks for null/empty values in critical composite key columns\n",
    "        and stores violating rows in a DQ table.\n",
    "        \"\"\"\n",
    "        cond = self.build_null_condition()\n",
    "\n",
    "        print(cond)\n",
    "\n",
    "        print (\"Invalid rows with null/empty values in key columns\")\n",
    "        df_invalid = df.filter(cond)\n",
    "        #print(df_invalid.schema.names)\n",
    "        df_invalid.show()\n",
    "        df_valid = df.filter(~cond)\n",
    "        print(\"Valid rows with no null/empty values in key columns\")\n",
    "        df_valid.show()\n",
    "\n",
    "        print(\"Total:\", df.count())\n",
    "        print(\"Invalid (nulls):\", df.filter(cond).count())\n",
    "        print(\"Valid:\", df.filter(~cond).count())\n",
    "        #print(df_invalid.count())\n",
    "        if df_invalid.count() > 0:\n",
    "            print(f\"Found {df_invalid.count()} rows with null/empty values in key columns.\")\n",
    "            df_invalid.write.mode(\"overwrite\").save_as_table(dq_table_name)\n",
    "\n",
    "        else:\n",
    "            print(\"No null/empty values found in key columns.\")\n",
    "        return df_invalid.count(), df_valid\n",
    "    \n",
    "    def duplicate_check(self, df, dq_table_name):\n",

    "        \"\"\"\n",
    "        Checks for duplicate rows based on composite key columns\n",
    "        and stores violating rows in a DQ table.\n",
    "        \"\"\"\n",
    "\n",
    "        # introduce row number to append only the first occurence of the duplicated records to clean and the rest to duplicates tables\n",
    "        df_full_dedup = df.with_column('rn', \n",
    "            (row_number().over(Window.partition_by([col(c) for c in self.key_columns]).order_by([col(c) for c in self.key_columns]))))\n",
    "        #df_full_dedup.show()\n",
    "        df_full_dedup_clean = df_full_dedup.filter(col('rn') == 1)\n",
    "        df_full_dedup_clean = df_full_dedup_clean.drop('rn')\n",
    "        df_full_dedup = df_full_dedup.filter(col('rn') != 1).drop('rn')\n",
    "        # To be passed to junk check \n",
    "        #df_clean_final = df_clean.union(df_full_dedup_clean)\n",
    "        df_full_dedup.show()\n",
    "\n",
    "        clean_count = df_full_dedup_clean.count()\n",
    "        print (f\"{clean_count} clean records\")\n",
    "\n",
    "        dupe_count = df_full_dedup.count()\n",
    "        \n",
    "        if dupe_count > 0:\n",
    "            print(f\"{dupe_count} duplicate rows found. Recording to {dq_table_name}\")\n",
    "            df_full_dedup.write.mode(\"overwrite\").save_as_table(dq_table_name)\n",
    "        else:\n",
    "            print(\"Duplicate check passed: No duplicate found\")\n",
    "\n",
    "        return df_full_dedup_clean\n",
    "\n",
    "    def junk_value_check(self, df, columns, dtype, min_valid_epoch=1600000000000):\n",
    "        \"\"\"\n",
    "        Flags rows where timestamp fields are present but invalid:\n",
    "        - Either not castable to integer\n",
    "        - Or below a minimum valid epoch threshold\n",
    "        \"\"\"\n",
    "        junk_condition = None\n",
    "\n",
    "        for column in columns:\n",
    "            trimmed_col = trim(col(column))\n",
    "            casted_col = try_cast(trimmed_col, IntegerType())\n",
    "            if dtype == \"timestamp_type\":\n",
    "                condition = ((casted_col.is_null()) | (casted_col < min_valid_epoch))\n",
    "            else :\n",
    "                condition = (casted_col.is_null())\n",
    "\n",
    "            junk_condition = condition if junk_condition is None else (junk_condition | condition)\n",
    "\n",
    "        return df.filter(junk_condition),df.filter(~junk_condition)\n",
    "    \n",
    "    def weather_junk_val_check (self,df,columns,dtype,weather_data_types):\n",
    "        junk_condition = None\n",
    "\n",
    "        for column in columns:\n",
    "            trimmed_col = trim(col(column))\n",
    "            \n",
    "            if dtype == \"timestamp_type\":\n",
    "                casted_col = try_cast(trimmed_col, TimestampType())\n",
    "                condition = (casted_col.is_null())\n",
    "                             \n",
    "            elif dtype == \"datatype_check\":\n",
    "                condition = ~trimmed_col.isin(weather_data_types)\n",
    "                \n",
    "            else:\n",
    "                print (\"Unrecognized dtype\")\n",
    "\n",
    "            junk_condition = condition if junk_condition is None else (junk_condition | condition)\n",
    "\n",
    "        return df.filter(junk_condition),df.filter(~junk_condition)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 31,

   "id": "622792c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "trip_key_cols = ['\"VendorID\"', '\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"', '\"PULocationID\"', '\"DOLocationID\"']\n",
    "weather_key_cols = ['DATE', 'DATATYPE']\n",
    "weather_data_types = [\"AWND\", \"WT01\", \"WSF5\", \"WSF2\", \"WDF5\", \"WDF2\", \"TMIN\", \"TMAX\", \"SNWD\", \"PRCP\", \"WT08\", \"SNOW\", \"WT03\", \"WT02\"]\n",
    "\n",
    "trip_ts_columns = ['\"tpep_pickup_datetime\"', '\"tpep_dropoff_datetime\"']\n",
    "trip_int_columns = ['\"VendorID\"', '\"PULocationID\"', '\"DOLocationID\"']\n",
    "\n",
    "weather_ts_columns = ['date']\n",
    "weather_data_columns = ['datatype']\n",
    "\n",
    "\n",
    "dq_table_name=\"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_NULL_RECORDS\"\n",
    "invalid_trip_data = \"INBOUND_INTEGRATION.DQ_TRIP.INVALID_TRIP_DATA\"\n",
    "valid_trip_data=\"INBOUND_INTEGRATION.SDS_TRIP.TRIP_DATA_VALIDATED\"\n",
    "trip_duplicates = \"INBOUND_INTEGRATION.DQ_TRIP.TRIP_DATA_DUPLICATES\"\n",
    "raw_trip_data = \"INBOUND_INTEGRATION.LANDING_TRIP.YELLOW_TRIP_RECORDS\"\n",
    "\n",
    "weather_null = \"INBOUND_INTEGRATION.DQ_WEATHER.WEATHER_NULL_RECORDS\"\n",
    "weather_duplicates = \"INBOUND_INTEGRATION.DQ_WEATHER.WEATHER_DATA_DUPLICATES\"\n",
    "valid_weather_data = \"INBOUND_INTEGRATION.SDS_WEATHER.WEATHER_DATA_VALIDATED\"\n",
    "invalid_weather_data = \"INBOUND_INTEGRATION.DQ_WEATHER.INVALID_WEATHER_DATA\"\n",
    "raw_weather_data=\"INBOUND_INTEGRATION.LANDING_WEATHER.NYC_WEATHER\"\n",
    "\n",
    "zone_lookup = \"INBOUND_INTEGRATION.LANDING_TRIP.TAXI_ZONE_LOOKUP\"\n",
    "final_table = \"OUTBOUND_INTEGRATION.TRIP_ANALYTICS.TRIP_ANALYTICS\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 32,

   "id": "f4d53565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col,min,to_date\n",
    "\n",
    "valid_weather_data = \"INBOUND_INTEGRATION.SDS_WEATHER.WEATHER_DATA_VALIDATED\"\n",
    "\n",
    "def pivot_weather_table():\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    df = session.table(valid_weather_data)\n",
    "    \n",
    "    datatypes = [row[0] for row in df.select (\"datatype\").distinct().collect()]\n",
    "    #print (datatypes[0])\n",
    "    #pivoted_df = df.pivot(datatypes).on(\"value\").group_by(\"date\").agg()\n",
    "\n",
    "    pivoted_df = (\n",
    "        df.group_by(\"DATE\")                       # <- group first\n",
    "        .pivot(\"datatype\",datatypes)              # <- then pivot on DATATYPE values\n",
    "        .agg(min(col(\"VALUE\")))             # <- single aggregate is fine (and typical)\n",
    "    )\n",
    "\n",
    "\n",
    "    pivoted_df=pivoted_df.with_column('\"ODate\"',to_date(col(\"DATE\"))\n",
    "                                    ).with_column_renamed(\"'TMIN'\",'\"Tmin\"'\n",
    "                                    ).with_column_renamed(\"'TMAX'\",'\"Tmax\"'\n",
    "                                    ).with_column_renamed(\"'PRCP'\",'\"Prcp\"'\n",
    "                                    ).with_column_renamed(\"'SNOW'\",'\"Snow\"'\n",
    "                                    ).with_column_renamed(\"'SNWD'\",'\"Snwd\"'\n",
    "                                    ).with_column_renamed(\"'AWND'\",'\"Awnd\"'\n",
    "                                    ).with_column_renamed(\"'WSF5'\",'\"Wsf5\"'\n",
    "                                    ).with_column_renamed(\"'WDF5'\",'\"Wdf5\"'\n",
    "                                    ).with_column_renamed(\"'WSF2'\",'\"Wsf2\"'\n",
    "                                    ).with_column_renamed(\"'WDF2'\",'\"Wdf2\"')\n",
    "\n",
    "    return pivoted_df.select('\"ODate\"','\"Tmin\"','\"Tmax\"','\"Prcp\"','\"Snow\"','\"Snwd\"','\"Awnd\"','\"Wsf5\"','\"Wdf5\"','\"Wsf2\"','\"Wdf2\"')#,\"'WT01'\",\"'WT02'\",\"'WT03'\",\"'WT08'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,

   "id": "66d8ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, to_timestamp,concat_ws\n",
    "\n",
    "valid_trip_data=\"INBOUND_INTEGRATION.SDS_TRIP.TRIP_DATA_VALIDATED\"\n",
    "\n",
    "def trip_records_transformer():\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    df = session.table(valid_trip_data)\n",
    "    #df.show()\n",
    "    transformed_df=(df.with_column('\"PickupTime\"',to_timestamp('\"tpep_pickup_datetime\"'))\n",
    "        .with_column('\"DropoffTime\"',to_timestamp('\"tpep_dropoff_datetime\"'))\n",
    "        .with_column('\"RideDate\"', to_date('\"tpep_pickup_datetime\"'))\n",
    "        .with_column_renamed('\"passenger_count\"', '\"PassengerCount\"')\n",
    "        .with_column_renamed('\"trip_distance\"', '\"TripDistance\"')\n",
    "        .with_column_renamed('\"fare_amount\"', '\"FareAmount\"')\n",
    "        .with_column_renamed('\"tip_amount\"', '\"TipAmount\"')\n",
    "        .with_column_renamed('\"tolls_amount\"', '\"TollsAmount\"')\n",
    "        .with_column_renamed('\"total_amount\"', '\"TotalAmount\"')\n",
    "        .with_column_renamed('\"Airport_fee\"', '\"AirportFee\"')\n",
    "        .with_column_renamed('\"cbd_congestion_fee\"', '\"CbdCongestionFee\"')\n",
    "        )\n",
    "    \n",
    "    return transformed_df.select('\"VendorID\"','\"RideDate\"','\"PickupTime\"','\"DropoffTime\"','\"TripDistance\"','\"PULocationID\"','\"DOLocationID\"','\"FareAmount\"','\"TipAmount\"','\"TollsAmount\"','\"TotalAmount\"') "
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 51,

   "id": "8f29343a",
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "250003 (08001): 404 Not Found: post https://OB52901.snowflakecomputing.com:443/session/v1/login-request?request_id=69eb2c40-aec1-4568-89a4-fd677d5e7ba3&databaseName=INBOUND_INTEGRATION&schemaName=LANDING_WEATHER&roleName=ACCOUNTADMIN&request_guid=dafc4338-7bba-4d9e-bc80-3ef5c7afb2f6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     final_df\u001b[38;5;241m=\u001b[39mfinal_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVendorID\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickupAddress\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropAddress\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickupTime\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropoffTime\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTripDistance\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotalAmount\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTmax\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrcp\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnow\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnwd\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAwnd\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWsf2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWdf2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWsf5\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     83\u001b[0m     final_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(final_table)\n\u001b[0;32m---> 85\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_parameters\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     df_trip_orig \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mtable(raw_trip_data)\n\u001b[1;32m      5\u001b[0m     df_weather \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mtable(raw_weather_data)\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/snowpark/session.py:505\u001b[0m, in \u001b[0;36mSession.SessionBuilder.create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m     _add_session(session)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconnection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_app_name:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_json:\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/snowpark/session.py:547\u001b[0m, in \u001b[0;36mSession.SessionBuilder._create_internal\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparamstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqmark\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m new_session \u001b[38;5;241m=\u001b[39m Session(\n\u001b[0;32m--> 547\u001b[0m     ServerConnection({}, conn) \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mServerConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/snowpark/_internal/server_connection.py:170\u001b[0m, in \u001b[0;36mServerConnection.__init__\u001b[0;34m(self, options, conn)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lower_case_parameters \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mlower(): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m options\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_application_parameters()\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lower_case_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_string_size \u001b[38;5;241m=\u001b[39m DEFAULT_STRING_SIZE\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39m_session_parameters:\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/__init__.py:54\u001b[0m, in \u001b[0;36mConnect\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(SnowflakeConnection\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mConnect\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SnowflakeConnection:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSnowflakeConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/connection.py:554\u001b[0m, in \u001b[0;36mSnowflakeConnection.__init__\u001b[0;34m(self, connection_name, connections_file_path, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m _get_default_connection_params()\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_error_attributes()\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_telemetry \u001b[38;5;241m=\u001b[39m TelemetryClient(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpired \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/connection.py:911\u001b[0m, in \u001b[0;36mSnowflakeConnection.connect\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(exceptions_dict))\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__open_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/connection.py:1312\u001b[0m, in \u001b[0;36mSnowflakeConnection.__open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;66;03m# okta URL, e.g., https://<account>.okta.com/\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_class \u001b[38;5;241m=\u001b[39m AuthByOkta(\n\u001b[1;32m   1307\u001b[0m         application\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplication,\n\u001b[1;32m   1308\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogin_timeout,\n\u001b[1;32m   1309\u001b[0m         backoff_generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backoff_generator,\n\u001b[1;32m   1310\u001b[0m     )\n\u001b[0;32m-> 1312\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthenticate_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_password \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# ensure password won't persist\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth_class\u001b[38;5;241m.\u001b[39mreset_secrets()\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/connection.py:1637\u001b[0m, in \u001b[0;36mSnowflakeConnection.authenticate_with_retry\u001b[0;34m(self, auth_instance)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauthenticate_with_retry\u001b[39m(\u001b[38;5;28mself\u001b[39m, auth_instance) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;66;03m# make some changes if needed before real __authenticate\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1637\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_authenticate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   1639\u001b[0m         \u001b[38;5;66;03m# cached id_token expiration error, we have cleaned id_token and try to authenticate again\u001b[39;00m\n\u001b[1;32m   1640\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID token expired. Reauthenticating...: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, ex)\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/connection.py:1669\u001b[0m, in \u001b[0;36mSnowflakeConnection._authenticate\u001b[0;34m(self, auth_instance)\u001b[0m\n\u001b[1;32m   1667\u001b[0m auth_instance\u001b[38;5;241m.\u001b[39m_retry_ctx\u001b[38;5;241m.\u001b[39mset_start_time()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1669\u001b[0m     \u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthenticate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarehouse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarehouse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpasscode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_passcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpasscode_in_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_passcode_in_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmfa_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mfa_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassword_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_password_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1684\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1685\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperational Error raised at authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1686\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor authenticator: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(auth_instance)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1687\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/auth/_auth.py:226\u001b[0m, in \u001b[0;36mAuth.authenticate\u001b[0;34m(self, auth_instance, account, user, database, schema, warehouse, role, passcode, passcode_in_password, mfa_callback, password_callback, session_parameters, timeout)\u001b[0m\n\u001b[1;32m    217\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     },\n\u001b[1;32m    223\u001b[0m )\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ForbiddenError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# HTTP 403\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\n\u001b[1;32m    235\u001b[0m         msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to connect to DB. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         sqlstate\u001b[38;5;241m=\u001b[39mSQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED,\n\u001b[1;32m    244\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/network.py:805\u001b[0m, in \u001b[0;36mSnowflakeRestful._post_request\u001b[0;34m(self, url, headers, body, token, external_session_id, timeout, socket_timeout, _no_results, no_retry, _include_retry_params)\u001b[0m\n\u001b[1;32m    802\u001b[0m     ret \u001b[38;5;241m=\u001b[39m probe_connection(full_url)\n\u001b[1;32m    803\u001b[0m     pprint(ret)\n\u001b[0;32m--> 805\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexternal_session_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_session_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_retry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_include_retry_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_include_retry_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43msocket_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msocket_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mret[code] = \u001b[39m\u001b[38;5;132;01m{code}\u001b[39;00m\u001b[38;5;124m, after post request\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    819\u001b[0m         code\u001b[38;5;241m=\u001b[39m(ret\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    820\u001b[0m     )\n\u001b[1;32m    821\u001b[0m )\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m MASTER_TOKEN_EXPIRED_GS_CODE:\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/network.py:924\u001b[0m, in \u001b[0;36mSnowflakeRestful.fetch\u001b[0;34m(self, method, full_url, headers, data, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m retry_ctx\u001b[38;5;241m.\u001b[39mset_start_time()\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_exec_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/network.py:1054\u001b[0m, in \u001b[0;36mSnowflakeRestful._request_exec_wrapper\u001b[0;34m(self, session, method, full_url, headers, data, retry_ctx, no_retry, token, external_session_id, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1052\u001b[0m         raise_raw_http_failure \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError)\n\u001b[1;32m   1053\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_retry:\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1055\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIgnored error\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/network.py:975\u001b[0m, in \u001b[0;36mSnowflakeRestful._request_exec_wrapper\u001b[0;34m(self, session, method, full_url, headers, data, retry_ctx, no_retry, token, external_session_id, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m raise_raw_http_failure \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise_raw_http_failure\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     return_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_exec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexternal_session_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_session_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_raw_http_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_raw_http_failure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m return_object\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/network.py:1259\u001b[0m, in \u001b[0;36mSnowflakeRestful._request_exec\u001b[0;34m(self, session, method, full_url, headers, data, token, external_session_id, catch_okta_unauthorized_error, is_raw_text, is_raw_binary, binary_data_handler, socket_timeout, is_okta_authentication, raise_raw_http_failure)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RetryRequest(err)\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/network.py:1208\u001b[0m, in \u001b[0;36mSnowflakeRestful._request_exec\u001b[0;34m(self, session, method, full_url, headers, data, token, external_session_id, catch_okta_unauthorized_error, is_raw_text, is_raw_binary, binary_data_handler, socket_timeout, is_okta_authentication, raise_raw_http_failure)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m raise_raw_http_failure:\n\u001b[1;32m   1207\u001b[0m             raw_ret\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m-> 1208\u001b[0m         \u001b[43mraise_failed_request_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_ret\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# required for tests\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/network.py:235\u001b[0m, in \u001b[0;36mraise_failed_request_error\u001b[0;34m(connection, url, method, response)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_failed_request_error\u001b[39m(\n\u001b[1;32m    230\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    231\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    232\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    233\u001b[0m     response: Response,\n\u001b[1;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInterfaceError\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreason\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmethod\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43murl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merrno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mER_FAILED_TO_REQUEST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msqlstate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/errors.py:279\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    258\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    287\u001b[0m             error_class,\n\u001b[1;32m    288\u001b[0m             error_value,\n\u001b[1;32m    289\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/errors.py:337\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/cleanenv310/lib/python3.10/site-packages/snowflake/connector/errors.py:210\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    208\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    211\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    212\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    213\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    214\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    215\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    216\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    218\u001b[0m     ),\n\u001b[1;32m    219\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    220\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    221\u001b[0m )\n",
      "\u001b[0;31mInterfaceError\u001b[0m: 250003 (08001): 404 Not Found: post https://OB52901.snowflakecomputing.com:443/session/v1/login-request?request_id=69eb2c40-aec1-4568-89a4-fd677d5e7ba3&databaseName=INBOUND_INTEGRATION&schemaName=LANDING_WEATHER&roleName=ACCOUNTADMIN&request_guid=dafc4338-7bba-4d9e-bc80-3ef5c7afb2f6"

     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    session = Session.builder.configs(connection_parameters).create()\n",
    "    df_trip_orig = session.table(raw_trip_data)\n",
    "    df_weather = session.table(raw_weather_data)\n",
    "    \n",
    "    weather_df = pivot_weather_table() \n",
    "    trip_df = trip_records_transformer()\n",
    "\n",
    "    dq = DQCheck(session, trip_key_cols, dq_table_name)\n",
    "\n",
    "    # Null Check\n",
    "    null_count, df_after_nulls = dq.null_check(df_trip_orig, session, dq_table_name)\n",
    "    if null_count == 0:\n",
    "        print(\"Proceeding to duplicate check\")\n",
    "    else:\n",
    "        print(\"Nulls found. Review before proceeding.\")\n",
    "\n",
    "    df_after_dupes = dq.duplicate_check(df_after_nulls,session,dq_table_name)\n",
    "\n",
    "    # Junk Check\n",
    "    df_junk_ts,df_clean_ts = dq.junk_value_check(df_after_dupes, trip_ts_columns, \"timestamp_type\")\n",
    "    df_junk_int,df_clean_int = dq.junk_value_check(df_clean_ts, trip_int_columns, \"integer_type\")\n",
    "\n",
    "    df_junk_int.write.mode(\"overwrite\").save_as_table(invalid_trip_data)\n",
    "    df_junk_ts.write.mode(\"append\").save_as_table(invalid_trip_data)\n",
    "    df_clean_int.write.mode(\"overwrite\").save_as_table(valid_trip_data)\n",
    "    print (f\"{df_clean_int.count()} Total valid trip records written into {valid_trip_data} table\")\n",
    "\n",
    "    ### Weather Data DQ Check ###\n",
    "    dq = DQCheck(session, weather_key_cols, dq_table_name)\n",
    "\n",
    "    # Null Check\n",

    "    null_count, df_after_nulls = dq.null_check(df_weather, weather_null)\n",

    "    if null_count == 0:\n",
    "        print(\"Proceeding to duplicate check\")\n",
    "    else:\n",
    "        print(\"Nulls found. Review before proceeding.\")\n",
    "\n",

    "    df_after_dupes = dq.duplicate_check(df_after_nulls, weather_null)\n",

    "\n",
    "    # Junk Check\n",
    "    df_junk_ts,df_clean_ts = dq.weather_junk_val_check(df_after_dupes, weather_ts_columns, \"timestamp_type\",weather_data_types )\n",
    "    df_junk_int,df_clean_int = dq.weather_junk_val_check(df_clean_ts, weather_data_columns, \"datatype_check\",weather_data_types)\n",
    "\n",
    "    df_junk_int.write.mode(\"overwrite\").save_as_table(invalid_weather_data)\n",
    "    df_clean_int.write.mode(\"overwrite\").save_as_table(valid_weather_data)\n",
    "    print (f\"{df_clean_int.count()} Total valid weather data records written into {valid_weather_data} table\")\n",
    "    \n",
    "    \n",
    "\n",
    "    z_lookup = session.table(zone_lookup)\n",
    "    z_pickup = z_lookup.select(\n",
    "        col(\"LOCATIONID\").alias(\"Pu_location_id\"),\n",
    "        col(\"BOROUGH\").alias(\"pu_borough\"),\n",
    "        col(\"zone\").alias(\"pu_zone\"))\n",
    "\n",
    "    z_drop = z_lookup.select(\n",
    "        col(\"LOCATIONID\").alias(\"do_location_id\"),\n",
    "        col(\"BOROUGH\").alias(\"do_borough\"),\n",
    "        col(\"zone\").alias(\"do_zone\"))\n",
    "\n",
    "\n",
    "    join_df = trip_df.join(z_pickup,\n",
    "                        trip_df['\"PULocationID\"']==z_pickup[\"Pu_location_id\"]\n",
    "                        ).join(z_drop,\n",
    "                               trip_df['\"DOLocationID\"']==z_drop[\"do_location_id\"]\n",
    "                               ).join(weather_df,\n",
    "                                      trip_df['\"RideDate\"'] == weather_df['\"ODate\"'])\n",
    "\n",
    "    final_df= join_df.with_column('\"PickupAddress\"',\n",
    "                                  concat_ws(lit(','),join_df[\"pu_zone\"],join_df[\"pu_borough\"])\n",
    "                                  ).with_column('\"DropAddress\"',\n",
    "                                                concat_ws(lit(','),join_df[\"do_zone\"],join_df[\"do_borough\"]))\n",
    "\n",
    "    # final_df = trip_df.join(\n",
    "    #     weather_df,\n",
    "    #     trip_df['\"RideDate\"'] == weather_df['\"ODate\"'],\n",
    "    #     join_type = \"inner\"\n",
    "    # )\n",
    "\n",
    "    final_df=final_df.select('\"VendorID\"','\"PickupAddress\"','\"DropAddress\"','\"PickupTime\"','\"DropoffTime\"','\"TripDistance\"','\"TotalAmount\"','\"Tmin\"','\"Tmax\"','\"Prcp\"','\"Snow\"','\"Snwd\"','\"Awnd\"','\"Wsf2\"','\"Wdf2\"','\"Wsf5\"','\"Wdf5\"')\n",
    "    final_df.write.mode(\"overwrite\").saveAsTable(final_table)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
